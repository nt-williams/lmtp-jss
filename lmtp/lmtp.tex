\documentclass[]{jss}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{sansmath}
\newcommand{\Q}{\mathsf{m}}
\renewcommand{\r}{\mathsf{r}}
\newcommand{\g}{\mathsf{g}}
\renewcommand{\P}{\mathsf{P}}
\newcommand{\p}{\mathsf{p}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\author{
Nicholas Williams, MPH\\Weill Cornell Medicine \And Iv\'an D\'iaz,
PhD\\Weill Cornell Medicine
}
\title{\pkg{lmtp}: An \proglang{R} Package for Machine Learning and Non-Parametric Causal
Effects for Longitudinal Studies}

\Plainauthor{Nicholas Williams, MPH, Iv\'an D\'iaz, PhD}
\Plaintitle{lmtp: An R Package for Machine Learning and Non-Parametric Causal
  Effects for Longitudinal Studies}
\Shorttitle{\pkg{lmtp}: Non-Parametric Causal
  Effects for Longitudinal Studies}

\Abstract{ The majority of causal inference methods consider treatment
  effects based on counterfactual outcomes where exposure is
  deterministically assigned. When exposure is continuous,
  deterministic interventions may be irrelevant and impossible to
  bring about. As a solution, modified treatment policies offer a
  generalization that allows for the study of feasible interventions
  and offer a safeguard against positivity violations. The \pkg{lmtp}
  package implements the estimators of
  \citet{diazNonparametricCausalEffects2020a} for estimating causal
  effects based on non-parametric modified treatment policies in
  \code{R}. In addition to modified treatment policies, the package
  can be used to estimate effects of deterministic and dynamic
  interventions. The methods provided can also be applied to both
  point-treatment and longitudinal settings, and can account for
  time-varying exposure, covariates, and right censoring, thereby
  providing a very general tool for causal inference in longitudinal
  studies. Additionally, two of the provided estimators are based on
  flexible machine learning regression algorithms, and avoid bias due
  to parametric model misspecification while maintaining valid
  statistical inference.}

\Keywords{Causal inference, longitudinal data, non-parametric,
  modified treatment policies, \proglang{R}} \Plainkeywords{Causal
  inference, longitudinal data, non-parametric, modified treatment
  policies, R}

%% publication information
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{}
%% \Acceptdate{2012-06-04}

\Address{
  Nicholas Williams and Iv\'an D\'iaz\\
  Division of Biostatistics\\
  Department of Population Health Sciences\\ 
  Weill Cornell Medicine\\
  402 East 67th Street, New York, NY 10065\\
  E-mail: \email{niw4001@med.cornell.edu}\\
}

% Pandoc header

\begin{document}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

Most modern causal inference methods consider the effects of a
treatment on a population mean outcome under interventions that set
the treatment value deterministically. For example, the average
treatment effect (ATE) considers the hypothetical difference in a
population mean outcome if a dichotomous exposure was applied to all
observations versus if it was applied to none. In the case of a
continuous exposure, interventions that set the exposure to a static
value deterministically are of little practical
relevance. Furthermore, the estimation of causal effects requires the
so called positivity assumption which states that all observations
have a greater than zero chance of experiencing the exposure value
under consideration
\citep{rosenbaumCentralRolePropensity1983}. This assumption is often
violated when evaluating the effects of deterministic interventions,
and is usually exacerbated with longitudinal data as the number of
time points grows.

 
Modified treatment policies (MTPs) are a class of stochastic treatment
regimes that can be formulated to avoid the above problems
\citep{munozPopulationInterventionCausal2012,
  haneuseEstimationEffectInterventions2013}. In a recent article
\citep{diazNonparametricCausalEffects2020a}, we generalized the
theoretical framework for estimation of the effect of MTPs to the
longitudinal setting, accounting for time-varying treatment,
covariates, and right-censoring of the outcome. Briefly, MTPs are
hypothetical interventions where the post-intervention value of
treatment can depend on the actual observed treatment level and the
unit's history. As such, MTPs are useful to assess the effect of
continuous treatments. For example,
\citet{haneuseEstimationEffectInterventions2013} assess the effect of
reducing surgery time by a predetermined amount (e.g., 5 minutes) for
lung cancer patients, where the reduction is carried out only for
those patients for whom the intervention is feasible. Furthermore,
MTPs generalize many important effect estimands, such as the effect of
a dynamic treatment rule in which the treatment level is assigned as a
function of a unit's history. For example, dynamic treatment rules, a
particular case of MTPs, may be used to estimate the effect of
policies such as switching HIV treatment once the CD4 T-cell count
goes below a predetermined threshold \citep{petersen2014delayed}. MTPs
also generalize many interesting causal effects such as the average
treatment effects, the causal risk ratio, and causal odds ratio. In
this article we describe how \pkg{lmtp} can be used for estimating the
causal effects of MTPs, and present examples on the use of the
software for several of the above cases.

The package \pkg{lmtp} implements four methods for estimating the
effects of MTPs. Two of these estimators, a targeted minimum-loss
based estimator \citep{laanTargetedLearningCausal2011a,
  laanTargetedMaximumLikelihood2006} and a sequentially doubly-robust
estimator (SDR) \citep{buckleyLinearRegressionCensored1979,
  fanCensoredRegressionLocal1994,
  vanderlaanUnifiedCrossValidationMethodology2003,
  rotnitzkyDoublyRobustEstimation2006, rubinDoublyRobustCensoring2006,
  kennedyNonparametricMethodsDoubly2017}, are multiply-robust. TMLE
and SDR are implemented using cross-fitting to allow for the use of
flexible machine learning regression methodology
\citep{diazNonparametricCausalEffects2020a}. The package may be
download from CRAN at \url{cran.r-project.org/package=lmtp}.

\hypertarget{notation-and-modified-treatment-policies}{%
\section{Notation and modified treatment
policies}\label{notation-and-modified-treatment-policies}}

\hypertarget{data-structure}{%
\subsection{Data structure}\label{data-structure}}

In this article, we will use the notation of
\citet{diazNonparametricCausalEffects2020a} with only slight
modifications.  Let \(i\) be the index of an observation from a data
set with \(n\) total units and \(t\) be the index of time for a total
number of time points \(\tau\).  The observed data for observation
\(i\) may be denoted as

\begin{equation}
Z_i = (W, L_1, A_1, L_2, A_2, ..., L_{\tau}, A_{\tau}, Y_{\tau + 1}) 
\end{equation}

where \(W\) denotes baseline covariates, \(L_t\) denotes time-varying
covariates, \(A_t\) denotes a vector of exposure and/or censoring variables and \(Y\)
denotes an outcome measured at the end of study follow-up. We observe \(n\)
i.i.d. copies of \(Z\) with distribution \(\Prob\). We use \(A_t = a_t\) to
denote a realization of a random variable. If right-censoring
exists, \(A_t\) can be adapted so that \(A_t = (A_{1, t}, A_{2, t})\)
where \(A_{1, t}\) equals one if an observation is still in the study at
time \(t\) and zero otherwise, and \(A_{2, t}\) denotes the exposure at
time \(t\). We use an overbar to indicate the history of a variable up
until time \(t\). We
then use \(H_t = (\bar{L}_t, \bar{A}_{t-1})\) to denote the history of
all variables up until just before \(A_t\).

\hypertarget{modified-treatment-policies}{%
\subsection{Modified treatment
policies}\label{modified-treatment-policies}}

We use the potential outcomes framework to define the causal effect of
interest using our established data structure. We consider a
hypothetical policy where \(\bar{A}\) is set to a regime \(d\) defined
as \(A^{d}_t = d_t(A_t, H^{d}_t)\), where
\(H^{d}_t = (\bar{L}_t, \bar{A}^{d}_{t - 1})\), for a set of
user-given regimes \(d_t:t \in \{1, ..., \tau\}\). The defining
characteristic that makes regime \(d_t\) a modified treatment policy
is that it depends on the \emph{natural value} of treatment
\(\bar{A}_t\), that is, the value that the treatment would have taken
under no intervention. However, when the function $d_t$ only depends
on $H_t$, the LMTP reduces to the \textit{dynamic treatment regimes}
studied in the literature. Furthermore, when $d_t$ is a constant that
and does not depend on either $A_t$ or $H_t$, then LMTPs reduce to the
conventional static rules studied in the causal inference literature
\citep[e.g.,][]{bang2005doubly, van2011targeted}. Below we present
examples of all these interventions.

First, consider a study of the effect of physical activity on
mortality in the elderly. Assume that each patient is monitored at
several time points, and that a measure of physical activity such as
the metabolic equivalent of task (MET) \citep{mendes2018metabolic} is
measured together with a number of lifestyle, health status, and
demographic variables. In this setup, a natural question to ask would
be ``what is the effect on mortality of an intervention that increases
physical activity by $\delta$ units for patients whose socioeconomic
and health status allows it?'' Formally, consider a longitudinal study
with loss-to-follow-up. Let \(A_t = (A_{1, t}, A_{2, t})\) where
\(A_{1, t}\) equals one if an observation is still in the study at
time \(t\) and zero otherwise, and \(A_{2, t}\) denote a continuous
exposure at time \(t\) that can be changed through some
intervention. A modified treatment policy that increases \(A_{2,t}\),
whenever it is feasible to do so, can be defined as

\begin{equation}\label{eq:mtp}
d_t(a_t,h_t)=
\begin{cases}
  (1, a_{2,t} + \delta_t) & \text{if } a_{2,t} + \delta_t \leq u_t(h_t)  \\
  (1, a_{2,t}) & \text{if } a_{2,t} + \delta_t > u_t(h_t)
\end{cases}
\end{equation}

where \(u_t(h_t)\) defines the maximum level of
physical activity allowed for a patient with characteristics
$h_t$. Note that we also consider an intervention on $A_{1,t}$ because
we are interested in a hypothetical world where there is no
loss-to-follow-up. In this case the hypothetical exposure after
intervention, \(A^{d}_t\) depends on the actually observed exposure,
\(A_t\). This is in contrast to a deterministic intervention where
\(A^{d}_t\) would be set to some pre-specified value with probability
one.


For dynamic treatment rules, consider a hypothetical longitudinal
study where two different antiviral treatments are administered to HIV
positive patients. Sometimes an antiviral drug works at first, until
the virus develops resistance, at which point it is necessary to
change the treatment regime. Assume we are interested in assessing a
policy with two treatments encoded as $A_t\in \{0,1\}$, and we want to
assess the effect of a regime that would switch the antiviral
treatment as soon as the CD4 T cell count drops bellow 300. Let
\(A_t = (A_{1, t}, A_{2, t})\) where \(A_{1, t}\) equals one if an
observation is still in the study at time \(t\) and zero otherwise,
and \(A_{2, t}\) denotes the treatment arm at time $t$. Let $L_t$
denote the CD4 T cell count at time $t$. In this case, one may decide
to assess the effect of the rule
\begin{equation}\label{eq:dyn}
  d_t(h_t)=
  \begin{cases}
    (1, 1 - a_{2,t-1}) & \text{if } l_t < 300  \\
    (1, a_{2,t-1}) & \text{if } l_t  \geq 300.
  \end{cases}
\end{equation}

In contrast to the previous rule (\ref{eq:mtp}), the dynamic treatment
rule (\ref{eq:dyn}) does not depend on the natural value of treatment
at time $t$, it only depends on the history. % This induces certain
% technicalities in the estimation procedure for true MTPs that depend
% on the natural value of treatment
% \citep{diazNonparametricCausalEffects2020a}. However, t
The software and methods presented here handle both cases seamlessly.

In the case of a single time point setting where the data structure is
$Z=(W,A,Y)$, it follows trivially from the above definitions that the
average treatment effect from a cross-sectional study, defined as
$\E[Y(1) - Y(0)]$, can be estimated using MTPs by simply letting
$\tau = 1$ and contrasting two MTPs $d(A)=1$ and $d(A)=0$. The
\pkg{lmtp} package presented in this article allows the contrast of
different MTPs using differences, ratios, and odds ratios. We provide
examples of each of these contrasts in \S \ref{examples} below.

In what follows we focus on estimating the the causal effect of MTP \(d\) on
outcome \(Y\), using \pkg{lmtp}, through the causal parameter
\begin{equation}
  \theta = \E\{Y(\bar A^d)\}\text{,}
\end{equation}
where \(Y(\bar A^d)\) is the counterfactual outcome in a world, where
possibly contrary to fact, each entry of \(\bar{A}\) was modified
according to the MTP \(d\). When \(Y\) is continuous, \(\theta\) is
the mean population value of \(Y\) under MTP \(d\); when \(Y\) is
dichotomous, \(\theta\) is the population proportion of event \(Y\)
under MTP \(d\). Similarly, when \(Y\) is the indicator of an event by
end of the study, \(\theta\) is defined as the cumulative incidence of
\(Y\) under MTP \(d\).

\hypertarget{identification}{%
\subsection{Identification}\label{identification}}

The ability to estimate \(\theta\) depends on the ability to identify
an expression for \(\theta\) as a function of the data generating
distribution \(\Prob\) using only the observed data \(Z\) and not
counterfactual variables $Y^d$. A full review of these identification
assumptions is outside the scope of this article but is presented in
our technical paper \citep{diazNonparametricCausalEffects2020a}.
Briefly, the following standard assumptions must hold

\newtheorem{assumption}{Assumption}

\begin{assumption}[Consistency]\label{ass:cons}
\(\bar{A} = \bar{a} \implies Y = Y(\bar{a})\) for all $\bar{a} \in \mathop{\mathrm{supp}}\bar{A}$ 
\end{assumption}
\begin{assumption}[Exchangeability]\label{ass:ex}
  $A_t \perp \!\!\! \perp Y(\bar{a}) | H_t$ for all
  $\bar{a} \in \mathop{\mathrm{supp}}\bar{A}$ and
  $t \in \{1, ..., \tau\}$
\end{assumption}
\begin{assumption}[Positivity]\label{ass:pos}
  If $(a_t, h_t) \in \mathop{\mathrm{supp}}\{A_t, H_t\}$ then
  $(d(a_t, h_t), h_t) \in \mathop{\mathrm{supp}}\{A_t, H_t\}$ for
  $t \in \{1, ..., \tau \}$
\end{assumption}

The consistency assumption states that the potential outcome for an
observation under their observed exposure is the value of the outcome
that we did actually observe. The
exchangeability assumption is often also referred to as the
no-unmeasured confounding assumption; it is satisfied if all common
causes of the exposure and outcome are measured and adjusted for. Of
particular importance to this article is the positivity assumption
which states that the distribution of the exposure under the MTP is
supported in the data. Concretely, in a study with a continuous
exposure and loss-to-follow-up, the positivity assumption states that
if an observation with covariate history \(h_t\) and exposure \(a_t\)
who was not lost-to-follow-up at time \(t\) exists then there is also
an observation with covariate history \(h_t\) who was not
lost-to-follow-up at time \(t\) but whose exposure was observed as
\(d(a_t, h_t)\) that also exists. This assumption is
often an issue when working with continuous exposures and/or multiple
time points in the context of static or dynamic interventions. One
strength of MTPs is that if the support of the exposure variable is known,
then they may be formulated to avoid violations of the positivity
assumption.%

Under these identification assumptions, the causal parameter $\theta$
is identified as follows. Set $\Q_{\tau+1}= Y$. For $t=\tau,\ldots,1$,
recursively define
\begin{equation}
  \Q_t:(a_t, h_t) \mapsto \E\left[\Q_{t+1}(A_{t+1}^d, H_{t+1})\mid
    A_t=a_t, H_t=h_t\right].\label{eq:defQ}
\end{equation} Then $\theta$ is identified as
$\E[\Q_1(A_1^d, L_1)]$.
In addition to the function $\Q_t$, the description of the estimation
methods below will benefit from the definition of the ratio bewteen
the density of the post-intervention exposure, $A_t^d$, and the
density of the observed exposure, $A_t$. That is, define $\g_t^d(a_t\mid
h_t)$ as the density of $A_t^d$ conditional on $H_t=h_t$ evaluated at
$h_t$. Likewise define $\g_t(a_t\mid h_t)$ as the conditional density
of $A_t$. Our estimation procedure will make use of the density ratio
defined as:
\[\r_t(a_t, h_t) = \frac{\g_t^d(a_t\mid
    h_t)}{\g_t(a_t\mid h_t)}.\]
This density ratio reduces to the well-known inverse probability
weights used in estimation of the ATE in the single-time point setting
where $Z=(W,A,Y)$ and $A$ is binary.

\hypertarget{estimating-modified-treatment-policy-effects}{%
\section{Estimating modified treatment policy
effects}\label{estimating-modified-treatment-policy-effects}}

\hypertarget{estimation-methods}{%
\subsection{Estimation methods}\label{estimation-methods}}

The \pkg{lmtp} package implements four estimation methods: a targeted
minimum-loss based estimator (TMLE), a sequential doubly-robust
estimator (SDR), an estimator based on the parametric G-formula, and an inverse
probability weighted (IPW) estimator. We will only describe the use of
TMLE, \code{lmtp_tmle}, and SDR, \code{lmtp_sdr}, as
their use is strongly suggested over the others based on their
advantageous theoretical properties which allow for machine learning
regression while maintaining the ability to compute valid confidence
intervals and p-values.

Targeted minimum-loss based estimation is a general framework for
constructing asymptotically linear estimators leveraging machine
learning, with an optimal bias-variance tradeoff for the target causal
parameter \citep{vanderLaanRose11, vanderLaanRose18}. In general, TMLE
is constructed from a factorization of observed data likelihood into
an outcome regression and an intervention mechanism. Using the outcome
regression, an initial estimate of the target parameter is constructed
and then \textit{de-biased} by a fluctuation that depends on a
function of the intervention mechanism. The sequential doubly-robust
estimator is based on a unbiased transformation of the efficient
influence function of the target estimand. For a thorough discussion
of TMLE and SDR for static, dynamic, and modified treatment policies,
we refer the reader to
\cite{van2011targeted,luedtke2017sequential,rotnitzky2017multiply,
  diazNonparametricCausalEffects2020a}.

TMLE and SDR require estimation of two nuisance parameters at each
time point: an outcome mechanism and an intervention mechanism. Both
TMLE and SDR are multiply-robust in that they allow certain
configurations of nuisance parameters to be inconsistently
estimated. Specifically, TMLE is considered \(\tau + 1\)-multiply robust in
that it allows for inconsistent estimation of all the intervention
mechanisms prior to any time point $t$, as long as all outcome
mechanisms after time $t$ are consistently estimated. SDR is
\(2^{\tau}\)-robust in that at each time point, estimation of at most either
the intervention mechanism or outcome mechanism is
allowed to be inconsistent. Both TMLE and SDR are efficient when all
the treatment mechanism and outcome regression are consistently
estimated at a given consistency rate, but the SDR has better
protection against model misspecification \citep[see][for more details]{luedtke2017sequential,rotnitzky2017multiply,
  diazNonparametricCausalEffects2020a}.

It is important to note that the SDR estimator can produce an estimate
\(\hat{\theta}\) outside of the bounds of the parameter space
(e.g., probability estimates outside $[0,1]$), while the TMLE
guarantees that the estimate is within bounds of the parameter
space. With this in mind and because for a single time-point TMLE and
SDR are equally robust, we recommend use of TMLE for the case of a
single time-point, while we recommend use of SDR for the longitudinal
setting. All examples in this article will demonstrate use of both
estimators.

\hypertarget{required-data-structure}{%
\subsection{Required data structure}\label{required-data-structure}}

Data is passed to \pkg{lmtp} estimators through the \code{data}
argument. Data should be in wide format with one column per variable
per time point under study (i.e., there should be one column for every
variable in \(Z\)). These columns do not have to be in any specific
order and the data set may contain variables that are not used in
estimation. The names of treatment variables, censoring variables,
baseline covariates, and time-varying covariates are specified using
the \code{trt}, \code{cens}, \code{baseline}, and \code{time_vary}
arguments respectively. The \code{trt}, \code{cens}, and
\code{baseline} arguments accept character vectors and the \code{trt}
and \code{cens} arguments should be ordered according to the
time-ordering of the data generating mechanism. The \code{time_vary}
argument accepts an unnamed list ordered according to the
time-ordering of the model with each index containing the name of the
time-varying covariates for the given time. The outcome variable is
specified through the \code{outcome} argument.

Estimators are compatible with continuous, dichotomous and survival
outcomes. In the case of a dichotomous or continuous outcome, only a
single variable name should be passed to the \code{outcome}
argument. For survival outcomes, a vector containing the names of the
intermediate outcome and final outcome variables, ordered according to
time, should be specified with the \code{outcome}
argument. Dichotomous and survival outcomes should be coded using
zero's and one's where one indicates the occurrence of an event and
zero otherwise. If working with a survival outcome, once an
observation experiences an outcome, all future outcome variables
should also be coded with a one. The \code{outcome_type} argument
should be set to \code{"continuous"} for continuous outcomes and
\code{"binomial"} for dichotomous and survival outcomes. If the study
is subject to loss-to-follow-up, the \code{cens} argument must be
provided. Censoring indicators should be coded using zero's and one's
where one indicates an observation is observed at the next time and
zero indicates loss-to-follow-up. Once an observation's censoring
status is switched to zero it cannot change back to one. Missing data
before an observation is lost-to-follow-up is not allowed; a
preprocessing step using multiple imputation is recommended for such
variables.

The \code{k} argument controls a Markov assumption on the data
generating mechanism. When \code{k = Inf}, the history \(H_t\) will be
constructed using all previous time-point variables while setting
\code{k} to any other value
will restrict \(H_t\) to time-varying covariates from time \(t - k -
1\) until $t-1$.
Baseline confounders are always included in \(H_t\). The
\code{create_node_list} function may be used to inspect how variables
will be used for estimation. It is specified with the same \code{trt},
\code{baseline}, \code{time_vary}, and \code{k} arguments as
\pkg{lmtp} estimators and is used internally to create a ``node list''
that encodes which variables should be used at each time point of
estimation. For example, consider a study with the observed data
structure

\begin{equation}
Z = (W_1, W_2, L_{1, 1}, L_{1, 2}, A_1, L_{2, 1}, L_{2, 2}, A_2, Y_3)
\end{equation}

We can translate this data structure to \proglang{R} with

\begin{CodeChunk}

\begin{CodeInput}
R> baseline <- c("W_1", "W_2")
R> trt <- c("A_1", "A_2")
R> time_vary <- list(c("L_11", "L_12"), 
R+                   c("L_21", "L_22"))
R> create_node_list(trt = trt, baseline = baseline, 
R+                  time_vary = time_vary, tau = 2)
\end{CodeInput}

\begin{CodeOutput}
$trt
$trt[[1]]
[1] "W_1"  "W_2"  "L_11" "L_12" "A_1" 

$trt[[2]]
[1] "W_1"  "W_2"  "L_11" "L_12" "L_21" "L_22" "A_1"  "A_2" 


$outcome
$outcome[[1]]
[1] "W_1"  "W_2"  "L_11" "L_12" "A_1" 

$outcome[[2]]
[1] "W_1"  "W_2"  "L_11" "L_12" "A_1"  "L_21" "L_22" "A_2" 
\end{CodeOutput}
\end{CodeChunk}

A list of lists is returned with the names of the variables in \(H_t\)
to be used for estimation of the outcome regression and the treatment
mechanism at every time \(t\).  Notice that variables \(A_1\) and
\(A_2\) are included in the list of variables used for estimation of
the treatment mechamism (\texttt{trt}). This is due to the fact that
the nuisance parameter for the treatment mechanism is the density
ratio $\r_t$, which is a function of \(A_1\) and \(A_2\).

The density ratio is estimated based on a classification trick using
an auxiliary variable \(\Lambda\) as a pseudo outcome and the
treatment as a predictor. We now briefly describe how this density
ratio estimation is done; the process is fully automated and hidden
from the software user. Specifically, the TMLE and SDR estimation
methods require estimation of the ratio of the densities of $A_t^d$
and $A_t$, conditional on the history $H_t$, defined as $\r_t$
above. This is achieved through computing the odds in a classification
problem in an augmented dataset with $2n$ observations where the
outcome is the auxiliary variable $\Lambda$ (defined below) and the
predictors are the variables $A_t$ and $H_t$. In the $2n$ augmented
data set, the data structure at time \(t\) is redefined as

\begin{equation}
(H_{\lambda, i, t}, A_{\lambda, i, t}, \Lambda_{\lambda, i} : \lambda = 0, 1; i = 1, ..., n)
\end{equation}

where \(\Lambda_{\lambda, i} = \lambda_i\) indexes duplicate values.
For all duplicated observations $\lambda\in\{0,1\}$ with the same $i$,
\(H_{\lambda, i, t}\) is the same. For $\lambda = 0$,
$A_{\lambda, i, t}$ equals the observed exposure values $A_{i, t}$,
whereas for $\lambda=1$, $A_{\lambda, i, t}$ equals the exposure
values under the MTP \(d\), namely \(A^{d}_t\). The classification
approach to density ratio estimation proceeds by estimating the
conditional probability that $\Delta=1$ in this dataset, and dividing
it by the corresponding estimate of the conditional probability that
$\Delta=0$. Specifically, denoting $\P^\lambda$ the distribution of
the data in the augmented dataset, we have:
\[\r_t(a_t, h_t) = \frac{\p^\lambda(a_t, h_t \mid \Lambda =
    1)}{\p^\lambda(a_t, h_t \mid \Lambda =
    0)}=\frac{\P^\lambda(\Lambda = 1\mid A_t=a_t,
    H_t=h_t)}{\P^\lambda(\Lambda = 0\mid A_t=a_t, H_t=h_t)}.\] Further
details on this algorithm may be found in our technical paper
\citep{diazNonparametricCausalEffects2020a}.

\hypertarget{creating-treatment-policies}{%
\subsection{Creating treatment policies}\label{creating-treatment-policies}}

Modified treatment policies and deterministic static/dynamic treatment rules are specified using the \code{shift} argument, which
accepts a user-defined function that returns a vector of exposure values
modified according to the policy of interest. Shift functions should take two
arguments, the first for specifying a data set and the second for
specifying the current exposure variable. For example, a possible MTP
may increase exposure by 2 units if the natural exposure value was below
5 units and do nothing otherwise. A shift function for this MTP would
look like

\begin{CodeChunk}

\begin{CodeInput}
R> function(data, trt) {
R+  (data[[trt]] < 5)*(data[[trt]] + 2) + (data[[trt]] >= 5)*data[[trt]]
R+ }
\end{CodeInput}

\end{CodeChunk}

This framework is flexible and allows for specifying complex treatment
regimes that can depend on time and covariates. In the case of a
binary exposure, two shift functions are installed with
the package: \code{static_binary_on} which sets \(A_{i, t} = 1\), and
\code{static_binary_off} which sets \(A_{i, t} = 0\).

\hypertarget{the-estimation-engine}{%
\subsection{The estimation engine}\label{the-estimation-engine}}

An attractive property of multiply-robust estimators is that they can
incorporate flexible machine-learning algorithms for the estimation of
nuisance parameters $\Q_t$ and $\r_t$ while remaining
\(\sqrt{n}\)-consistent. The super learner algorithm is an ensemble
learner than incorporates a set of candidate models through a weighted
convex-combination based on cross-validation
\citep{laanSuperLearner2007}. Asymptotically, this weighted
combination of models, called the meta-learner, will outperform any
single one of its components.

Our package uses the implementation of the super learner provided by
the \pkg{sl3} package \citep{coyleSl3}. Analysts must create \pkg{sl3}
learner stacks which are then included in \code{lmtp_tmle} and
\code{lmtp_sdr} calls with the \code{lrnrs_trt} and
\code{lrnrs_outcome} arguments. The outcome variable type should guide
users on selecting the appropriate candidate learners for use with the
\code{lrnrs_outcome} argument. Regardless of whether an exposure is
continuous, dichotomous, or categorical, the exposure mechanism is
estimated using classification as discussed above, users should thus
only include candidate learners capable of binary classification with
the \code{lrnrs_trt} argument.

Candidate learners that rely on cross-validation
for the tuning of hyper-parameters should support grouped data if used with \code{lrnrs_trt}. Because
estimation of the treatment mechanism relies on the augmented \(2n\) duplicated
data set, duplicated observations must be put into the same fold
during sample-splitting. This is done automatically by the package.

Users may install \pkg{sl3} from \url{https://github.com/tlverse/sl3}.
Because \pkg{sl3} is not available for installation from a standard
repository at the time of writing of this manuscript, it is not
required to use \pkg{lmtp}. Instead, the \code{lrnrs_trt} and
\code{lrnrs_outcomes} arguments can be set equal to \code{NULL} and
nuisance parameters will be estimated using a generalized linear model
(GLM) with the \code{glm} function from the \pkg{stats} package
\citep{rcore}.

\hypertarget{additional-arguments}{%
\subsection{Additional arguments}\label{additional-arguments}}

Sample-splitting and cross-fitting is used with all methods to avoid
certain technical conditions that may not hold for machine learning
estimators \citep{zhengCrossValidatedTargetedMinimumLossBased2011b,
  chernozhukovDoubleDebiasedMachine2018}. Specifically, the data are
split in $V$ volds and each nuisance parameter is estimated $V$ times,
excluding data in one fold each time. The number of folds $V$ can be
set with the \code{folds} argument; the minimum number of allowed
folds is two.  If data has a hierarchical structure, the \code{id}
argument is used to indicate the name of a variable in the data set
indicating unique groups. These identifiers will be used for
generation of cross-validation folds and will be accounted for in
standard error calculations. If a continuous outcome has known bounds,
these bounds may be specified using the \code{bounds} argument with a
length two numeric vector where the first index is the lower bound and
the second index is the upper bound.

\hypertarget{contrasts}{%
\subsection{Contrasts}\label{contrasts}}

In addition to estimating a single MTP effect, researchers may be
interested in contrasting several MTPs. Contrasts between different
policies are implemented in the \code{lmtp_contrast} function. Users
may specify any number of objects returned by calls to
\code{lmtp_tmle} or \code{lmtp_sdr} to be compared to a single a
reference value or a single reference MTP, specified using the
\code{ref} argument.  Depending on the outcome type, contrasts may be
either additive (\code{type = "additive"}), an odds ratio (\code{type
  = "or"}), or the relative risk (\code{type = "rr"}).

\hypertarget{examples}{%
\subsection{Examples}\label{examples}}

\hypertarget{example-1-longitudinal-mtp-with-no-loss-to-follow-up}{%
\subsubsection{Example 1: Longitudinal MTP with no
loss-to-follow-up}\label{example-1-longitudinal-mtp-with-no-loss-to-follow-up}}

We have simulated data on \(n = 5000\) observations over a 5-month
period. Each observation has a continuous exposure (\code{A_1},
\code{A_2}, \code{A_3}, \code{A_4}) and covariate (\code{L_1},
\code{L_2}, \code{L_3}, \code{L_4}) recorded at months one through four
and a dichotomous outcome (\code{Y}) at month five. We assume no
loss-to-follow-up and no Markov assumption. This data set is installed
with the package and is stored in the object \code{sim_t4}.

For this example, we are interested in the effect of a longitudinal MTP
where at each month an observation's exposure decreases by one only if
their observed exposure wouldn't be less than one if modified. Our data
structure has no baseline confounders and we will use only GLMs for
estimation so the only objects we must specify are the treatment
variables, the time-varying covariates, the outcome variable, and the
MTP shift function.

\begin{CodeChunk}

\begin{CodeInput}
R> x <- c("A_1", "A_2", "A_3", "A_4")
R> tv <- list(c("L_1"), c("L_2"), c("L_3"), c("L_4"))
R> y <- "Y"
R> shift <- function(data, trt) {
R+   (data[[trt]] - 1) * (data[[trt]] - 1 >= 1) + 
R+     data[[trt]] * (data[[trt]] - 1 < 1)
R+ }
R> lmtp_tmle(sim_t4, x, y, time_vary = tv, shift = shift)
\end{CodeInput}

\begin{CodeOutput}
LMTP Estimator: TMLE
   Trt. Policy: (shift)

Population intervention effect
      Estimate: 0.2648
    Std. error: 0.0189
        95% CI: (0.2277, 0.3018)
\end{CodeOutput}

\begin{CodeInput}
R> lmtp_sdr(sim_t4, x, y, time_vary = tv, shift = shift)
\end{CodeInput}

\begin{CodeOutput}
LMTP Estimator: SDR
   Trt. Policy: (shift)

Population intervention effect
      Estimate: 0.2612
    Std. error: 0.0187
        95% CI: (0.2246, 0.2978)
\end{CodeOutput}

\end{CodeChunk}

\hypertarget{example-2-longitudinal-mtp-right-censoring-and-the-super-learner}{%
\subsubsection{Example 2: Longitudinal MTP, right-censoring, and the
super
learner}\label{example-2-longitudinal-mtp-right-censoring-and-the-super-learner}}

For this
example, we have a simulated dataset of \(n = 1000\) observations. Data
was simulated for three time points with a continuous time-varying
exposure at times \(t \in \{1, 2\}\) (\code{A1}, \code{A2}), a
dichotomous time-varying covariate at times \(t \in \{1, 2\}\)
(\code{L1}, \code{L2}), and a dichotomous outcome (\code{Y}) at time
\(\tau + 1 = 3\). Loss-to-follow-up is present after time \(t = 1\) so the data set
contains censoring indicators (\code{C1}, \code{C2}). This data is
installed with the package and is stored in the object \code{sim_cens}.

Suppose we are interested in the additive effect of an MTP where exposure
is increased by 0.5 at every time point for all observations. Instead of using a linear model, 
we will estimate the outcome regression and treatment mechanism using a super learner 
composed of a GLM, a random forest \citep{wrightRanger}, 
and multivariate adaptive regression splines \citep{milborrowEarth}. 

\begin{CodeChunk}

\begin{CodeInput}
R> x <- c("A1", "A2")
R> cen <- c("C1", "C2")
R> tv <- list(c("L1"), c("L2"))
R> y <- "Y"
R> shift <- function(data, trt) {
R+   data[[trt]] + 0.5
R+ }
R> lrnrs <- make_learner_stack(Lrnr_glm,
R+                             Lrnr_ranger, 
R+                             Lrnr_earth)
R> tml <- lmtp_tmle(sim_cens, x, y, time_vary = tv, 
R+                  cens = cen, shift = shift, learners_trt = lrnrs, 
R+                  learners_outcome = lrnrs, folds = 3)
R> print(tml)
\end{CodeInput}

\begin{CodeOutput}
LMTP Estimator: TMLE
   Trt. Policy: (shift)

Population intervention effect
      Estimate: 0.9013
    Std. error: 0.0095
        95% CI: (0.8827, 0.92)
\end{CodeOutput}

\begin{CodeInput}
R> sdr <- lmtp_sdr(sim_cens, x, y, time_vary = tv, 
R+                 cens = cen, shift = shift, learners_trt = lrnrs, 
R+                 learners_outcome = lrnrs, folds = 3)
R> print(sdr)
\end{CodeInput}

\begin{CodeOutput}
LMTP Estimator: SDR
   Trt. Policy: (shift)

Population intervention effect
      Estimate: 0.899
    Std. error: 0.0096
        95% CI: (0.8802, 0.9178)
\end{CodeOutput}

\end{CodeChunk}

If loss-to-follow-up exists, we can estimate the population mean
outcome under the observed exposures adjusting by informative
loss-to-follow-up by specifying \code{shift = NULL}. This estimate can
then be used as the reference value for calculating the additive
effect of the MTP compared to the observed exposures.

\begin{CodeChunk}

\begin{CodeInput}
R> tml_obs <- lmtp_tmle(sim_cens, x, y, time_vary = tv, 
R+                      cens = cen, shift = NULL, learners_trt = lrnrs, 
R+                      learners_outcome = lrnrs, folds = 3)
R> lmtp_contrast(tml, ref = tml_obs)
\end{CodeInput}

\begin{CodeOutput}
  LMTP Contrast: additive
Null hypothesis: theta == 0

  theta shift   ref std.error conf.low conf.high p.value
1 0.104 0.901 0.798   0.00617   0.0917     0.116  <0.001
\end{CodeOutput}

\begin{CodeInput}
R> sdr_obs <- lmtp_sdr(sim_cens, x, y, time_vary = tv, 
R+                     cens = cen, shift = NULL, learners_trt = lrnrs, 
R+                     learners_outcome = lrnrs, folds = 3)
R> lmtp_contrast(sdr, ref = sdr_obs)
\end{CodeInput}

\begin{CodeOutput}
  LMTP Contrast: additive
Null hypothesis: theta == 0

  theta shift   ref std.error conf.low conf.high p.value
1 0.101 0.899 0.798   0.00611   0.0886     0.113  <0.001
\end{CodeOutput}

\end{CodeChunk}

\hypertarget{example-3-survival-analysis-and-deterministic-effects}{%
\subsubsection{Example 3: Survival analysis and deterministic effects}\label{example-3-survival-analysis-and-deterministic-effects}}

The \pkg{lmtp} package may also be used to estimate deterministic causal effects, such as the causal relative risk.
Suppose we have time-to-event data on \(n = 2000\) observations with a time-invariant dichotomous exposure followed for a
period of seven days. We wish to estimate the causal relative risk of experiencing the event by day seven. This data is
installed with the package and is stored in the object \code{sim_point_surv}. 

\begin{CodeChunk}

\begin{CodeInput}
R> x <- "trt"
R> w <- c("W1", "W2")
R> cen <- paste0("C.", 0:5)
R> y <- paste0("Y.", 1:6)
R> tml1 <- lmtp_tmle(sim_point_surv, x, y, w, cens = cen, 
R+                   learners_trt = lrnrs, learners_outcome = lrnrs,
R+                   shift = static_binary_on, folds = 3)
R> tml0 <- lmtp_tmle(sim_point_surv, x, y, w, cens = cen, 
R+                   learners_trt = lrnrs, learners_outcome = lrnrs,
R+                   shift = static_binary_off, folds = 3)
R> lmtp_contrast(tml1, ref = tml0, type = "rr")
\end{CodeInput}

\begin{CodeOutput}
  LMTP Contrast: relative risk
Null hypothesis: theta == 1

  theta shift   ref std.error conf.low conf.high p.value
1  1.22  0.81 0.664    0.0339     1.14       1.3  <0.001
\end{CodeOutput}

\begin{CodeInput}
R> sdr1 <- lmtp_sdr(sim_point_surv, x, y, w, cens = cen, 
R+                 learners_trt = lrnrs, learners_outcome = lrnrs,
R+                 shift = static_binary_on, folds = 3)
R> sdr0 <- lmtp_sdr(sim_point_surv, x, y, w, cens = cen, 
R+                 learners_trt = lrnrs, learners_outcome = lrnrs,
R+                 shift = static_binary_off, folds = 3)
R> lmtp_contrast(sdr1, ref = sdr0, type = "rr")
\end{CodeInput}

\begin{CodeOutput}
  LMTP Contrast: relative risk
Null hypothesis: theta == 1

  theta shift   ref std.error conf.low conf.high p.value
1  1.21 0.812 0.671     0.034     1.13      1.29  <0.001
\end{CodeOutput}

\end{CodeChunk}

\hypertarget{example-4-dynamic-treatment-regimes}{%
\subsubsection{Example 4: Dynamic treatment regimes}\label{example-4-dynamic-treatment-regimes}}

Dynamic treatment regimes are treatment rules where treatment is applied based on a fixed rule that depends on covariate history. The \pkg{lmtp} package is capable of estimating the effects of deterministic dynamic treatment rules and modified treatment policies that depend on covariate history. 

For this example we will use the same data from example 1. However, we will extend the longitudinal MTP used in that example so that a shift in the exposure also depends on covariate history and time. Specifically, if $t = 1$ exposure decreases by one if an observation's realized exposure wouldn’t be less than one if modified. If $t > 1$ and the value of the time-varying covariate at that time-point equals one then exposure should decrease by one if an observation's realized exposure wouldn’t be less than one if modified; otherwise exposure shouldn't be modified.

\begin{CodeChunk}

\begin{CodeInput}
R> x <- c("A_1", "A_2", "A_3", "A_4")
R> tv <- list(c("L_1"), c("L_2"), c("L_3"), c("L_4"))
R> y <- "Y"
R> shift <- function(data, trt) {
R+   (data[[trt]] - 1) * (data[[trt]] - 1 >= 1) + 
R+     data[[trt]] * (data[[trt]] - 1 < 1)
R+ }
R> dynamic_shift <- function(data, trt) {
R+   if (trt == "A_1") {
R+     shift(data, trt)
R+   } else {
R+     ifelse(data[[sub("A", "L", trt)]] == 1, 
R+            shift(data, trt),
R+            data[[trt]])
R+   }
R+ }
R> lmtp_tmle(sim_t4, x, y, time_vary = tv, shift = dynamic_shift, folds = 3, 
R+           learners_outcome = lrnrs, learners_trt = lrnrs)
\end{CodeInput}

\begin{CodeOutput}
LMTP Estimator: TMLE
   Trt. Policy: (dynamic_shift)

Population intervention effect
      Estimate: 0.3246
    Std. error: 0.0097
        95% CI: (0.3055, 0.3437)
\end{CodeOutput}

\begin{CodeInput}
R> lmtp_sdr(sim_t4, x, y, time_vary = tv, shift = dynamic_shift, folds = 3, 
R+          learners_outcome = lrnrs, learners_trt = lrnrs)
\end{CodeInput}

\begin{CodeOutput}
LMTP Estimator: SDR
   Trt. Policy: (dynamic_shift)

Population intervention effect
      Estimate: 0.3177
    Std. error: 0.0099
        95% CI: (0.2983, 0.3371)
\end{CodeOutput}

\end{CodeChunk}

\hypertarget{extra-features}{%
\subsection{Extra features}\label{extra-features}}

Computation time can rapidly increase with many time points and when using the super learner. As a solution, \pkg{lmtp} 
can utilize parallel processing provided by the \pkg{future} package \citep{future}. In addition, 
\pkg{lmtp} is compatible with the \pkg{progressr} package \citep{progressr} for producing progress bars by wrapping estimator calls in
\code{with_progress}. For users familiar with the \pkg{broom} package \citep{broom}, \pkg{lmtp} contains a \code{tidy} method. 

\hypertarget{reference-manual}{%
\section{Reference Manual}\label{reference-manual}}

\subsection[lmtp_tmle and lmtp_sdr]{\code{lmtp\_tmle} and \code{lmtp\_sdr}}

\subsubsection{Arguments}

\begin{itemize}

  \item \code{data}: A data frame in wide format. 
  \item \code{trt}: A vector containing the column names of the treatment variables ordered by time.
  \item \code{outcome}: The column name of the outcome variable. In the case of time-to-event 
  analysis, a vector containing the column names of the intermediate outcome variables and
  the final outcome variable ordered by time. Only numeric values are allowed. If
  the outcome type is binary, data should be coded as zeroes and ones.
  \item \code{baseline}: An optional vector containing the columns names of baseline covariates
  to be included for adjustment at every time-point.
  \item \code{time_vary}: A list the same length as the number of time-points under observation.
  The list should be ordered following the time ordering of the model.
  Each index of the list should be a vector containing the column names of the time-varying covariates
  at that time-point. 
  \item \code{cens}: An optional vector, the same length as \code{time_vary}, containing
  the column names of censoring indicators. Must be provided if there is missingness in the outcome or
  if a time-to-event analysis.
  \item \code{shift}: A two argument function that specifies how treatment variables should be shifted.
  \item \code{k}: An integer controlling the Markov property of the data generating mechanism. If \code{k = Inf}
  (default) the history \(H_t\) will contain all previous time-point variables. If \code{k = 0} the history will
  only contain baseline variables and time-varying covariates at \(t - 1\).
  \item \code{outcome_type}: The outcome variable type. Valid options are \code{"continuous"} and \code{"binomial"}.
  \item \code{id}: An optional column name containing cluster-level identifiers.
  \item \code{bounds}: An optional vector of length two containing the upper and lower bounds
  for a continuous outcome. If \code{NULL} the bounds will be taken as the minimum and maximum
   of the observed data; ignored if \code{outcome_type = "binomial"}
  \item \code{learners_outcome}: An optional \pkg{sl3} learner stack to be used for estimation
  of the outcome regression. If \code{NULL}, estimation will default to using a generalized linear model.
  \item \code{learners_trt}: An optional \pkg{sl3} learner stack to be used for estimation
  of the treatment mechanism. If \code{NULL}, estimation will default to using a generalized linear model.
  \item \code{folds}: The number of folds to be used for cross-fitting. The minimum number of allowed folds is two.
  \item \code{bound}: Determines that maximum and minimum values (scaled) predictions will be
  bounded to. The default is \code{1e-5}, bounding predictions between \(1\times10^{-5}\) and \(0.9999\).
  
\end{itemize}

\subsubsection{Returns}

Objects returned from calls to \code{lmtp_tmle} or \code{lmtp_sdr} will contain:

\begin{itemize}

  \item \code{estimator}: The estimator used, either TMLE or SDR.
  \item \code{theta}: The estimated population MTP effect.
  \item \code{standard_error}: The estimated, influence function based, standard error of
  the MTP effect.
  \item \code{low}: The lower bound of the 95\% confidence interval of the MTP effect.
  \item \code{high}: The lower bound of the 95\% confidence interval of the MTP effect.
  \item \code{eif}: The estimated, uncentered, influence function.
  \item \code{shift}: The shift function specified with the \code{shift} argument.
  \item \code{outcome_reg}: An \(n \times \tau + 1\) matrix contained the outcome regression
  predictions. The mean of the first column is used for calculating \code{theta}.
  \item \code{density_ratios}: An \(n \times \tau\) matrix containing the estimated density
  ratios from estimation of the treatment mechanism.
  \item \code{weights_m}: A list the same length as the \code{folds} argument containing the
  super learner ensemble weights at each time-point for each fold of the outcome regression.
  \item \code{weights_r}: A list the same length as the \code{folds} argument containing the
  super learner ensemble weights at each time-point for each fold of the treatment mechanism
  estimation.
  \item \code{outcome_type}: The outcome variable type.
  
\end{itemize}

\subsection[lmtp_contrast]{\code{lmtp\_contrast}}

\subsubsection{Arguments}

\begin{itemize}

  \item \code{...}: One or more objects returned from calls to \code{lmtp_tmle} or \code{lmtp_sdr}.
  \item \code{ref}: Either a scalar reference value or another object returned from a call
  to \code{lmtp_tmle} or \code{lmtp_sdr}. \code{ref} will be compared to all other objects
  specified in the \code{...} argument.
  \item \code{type}: The contrast of interest. Valid options are \code{"additive"} (default)
  for the additive effect, \code{"rr"} for the relative risk, and \code{"or"} for the odds
  ratio. \code{"rr"} and \code{"or"} are only allowed when the outcome is dichotomous.
  
\end{itemize}

\subsubsection{Returns}

Objects returned from calls to \code{lmtp_contrast} will be a list containing:

\begin{itemize}

  \item \code{type}: The contrast type specified with the \code{type} argument.
  \item \code{null}: The null hypothesis.
  \item \code{vals}: A data frame with the number of rows equal to the number
  of objects specified in the \code{...} argument. The data frame will contain
  columns for the contrast estimate (\code{"theta"}), standard error (\code{std.error}),
  and 95\% confidence interval lower (\code{"conf.low"}) and upper (\code{"conf.high"}) bounds.
  \item \code{eifs}: The estimated, uncentered, influence functions of the contrast estimates.
  
\end{itemize}

\subsection[create_node_list]{\code{create\_node\_list}}

\subsubsection{Arguments}

\begin{itemize}

  \item \code{trt}: A vector containing the names of the treatment variables ordered by time.
  \item \code{tau}: An integer specifying the maximum time-point of the data generating mechanism.
  \item \code{time_vary}: A list the same length as the number of time-points under observation.
  The list should be ordered following the time ordering of the model.
  Each index of the list should be a vector containing the names of the time-varying covariates
  at that time-point. 
  \item \code{baseline}: An optional vector containing the names of baseline covariates
  to be included for adjustment at every time-point.
  \item \code{k}: An integer controlling the Markov property of the data generating mechanism. If \code{k = Inf}
  (default) the history \(H_t\) will contain all previous time-point variables. If \code{k = 0} the history will
  only contain baseline variables and time-varying covariates at \(t - 1\).
  
\end{itemize}

\subsubsection{Returns}

A list of length two. Each index of the list will contain a list of length \(\tau\) with
each index being a vector of the column names to be used for estimation at the corresponding
time-point of either the outcome regression or treatment mechanism.

\renewcommand\refname{References}
\bibliography{lmtp.bib}

\end{document}
