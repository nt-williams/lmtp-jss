
@article{kimIncrementalInterventionEffects2019,
	title = {Incremental {Intervention} {Effects} in {Studies} with {Many} {Timepoints}, {Repeated} {Outcomes}, and {Dropout}},
	url = {http://arxiv.org/abs/1907.04004},
	abstract = {Modern longitudinal studies feature data collected at many timepoints, often of the same order of sample size. Such studies are typically affected by dropout and positivity violations. We tackle these problems by generalizing effects of recent incremental interventions (which shift propensity scores rather than set treatment values deterministically) to accommodate multiple outcomes and subject dropout. We give an identifying expression for incremental effects when dropout is conditionally ignorable (without requiring treatment positivity), and derive the nonparametric efficiency bound for estimating such effects. Then we present efficient nonparametric estimators, showing that they converge at fast parametric rates and yield uniform inferential guarantees, even when nuisance functions are estimated flexibly at slower rates. We also study the efficiency of incremental effects relative to more conventional deterministic effects in a novel infinite time horizon setting, where the number of timepoints grows with sample size, and show that incremental effects yield near-exponential gains in this setup. Finally we conclude with simulations and apply our methods in a study of the effect of low-dose aspirin on pregnancy outcomes.},
	urldate = {2020-06-26},
	journal = {arXiv:1907.04004 [stat]},
	author = {Kim, Kwangho and Kennedy, Edward H. and Naimi, Ashley I.},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.04004},
	keywords = {Statistics - Methodology, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/Nick/Zotero/storage/X6HN2GWJ/Kim et al. - 2019 - Incremental Intervention Effects in Studies with M.pdf:application/pdf;arXiv.org Snapshot:/Users/Nick/Zotero/storage/K5GPSKHR/1907.html:text/html}
}

@article{kennedyNonparametricCausalEffects2019,
	title = {Nonparametric {Causal} {Effects} {Based} on {Incremental} {Propensity} {Score} {Interventions}},
	volume = {114},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1422737},
	doi = {10.1080/01621459.2017.1422737},
	abstract = {Most work in causal inference considers deterministic interventions that set each unit’s treatment to some fixed value. However, under positivity violations these interventions can lead to nonidentification, inefficiency, and effects with little practical relevance. Further, corresponding effects in longitudinal studies are highly sensitive to the curse of dimensionality, resulting in widespread use of unrealistic parametric models. We propose a novel solution to these problems: incremental interventions that shift propensity score values rather than set treatments to fixed values. Incremental interventions have several crucial advantages. First, they avoid positivity assumptions entirely. Second, they require no parametric assumptions and yet still admit a simple characterization of longitudinal effects, independent of the number of timepoints. For example, they allow longitudinal effects to be visualized with a single curve instead of lists of coefficients. After characterizing incremental interventions and giving identifying conditions for corresponding effects, we also develop general efficiency theory, propose efficient nonparametric estimators that can attain fast convergence rates even when incorporating flexible machine learning, and propose a bootstrap-based confidence band and simultaneous test of no treatment effect. Finally, we explore finite-sample performance via simulation, and apply the methods to study time-varying sociological effects of incarceration on entry into marriage. Supplementary materials for this article are available online.},
	language = {en},
	number = {526},
	urldate = {2020-06-26},
	journal = {Journal of the American Statistical Association},
	author = {Kennedy, Edward H.},
	month = apr,
	year = {2019},
	pages = {645--656},
	file = {Kennedy - 2019 - Nonparametric Causal Effects Based on Incremental .pdf:/Users/Nick/Zotero/storage/HXWFQT4N/Kennedy - 2019 - Nonparametric Causal Effects Based on Incremental .pdf:application/pdf}
}

@article{munozPopulationInterventionCausal2012,
	title = {Population intervention causal effects based on stochastic interventions},
	volume = {68},
	issn = {1541-0420},
	doi = {10.1111/j.1541-0420.2011.01685.x},
	abstract = {Estimating the causal effect of an intervention on a population typically involves defining parameters in a nonparametric structural equation model (Pearl, 2000, Causality: Models, Reasoning, and Inference) in which the treatment or exposure is deterministically assigned in a static or dynamic way. We define a new causal parameter that takes into account the fact that intervention policies can result in stochastically assigned exposures. The statistical parameter that identifies the causal parameter of interest is established. Inverse probability of treatment weighting (IPTW), augmented IPTW (A-IPTW), and targeted maximum likelihood estimators (TMLE) are developed. A simulation study is performed to demonstrate the properties of these estimators, which include the double robustness of the A-IPTW and the TMLE. An application example using physical activity data is presented.},
	language = {eng},
	number = {2},
	journal = {Biometrics},
	author = {Muñoz, Iván Díaz and van der Laan, Mark},
	month = jun,
	year = {2012},
	pmid = {21977966},
	pmcid = {PMC4117410},
	keywords = {Aged, Biometry, Causality, Computer Simulation, Data Interpretation, Statistical, Health Promotion, Humans, Likelihood Functions, Mortality, Motor Activity, Stochastic Processes},
	pages = {541--549},
	file = {Full Text:/Users/Nick/Zotero/storage/KALSGSEI/Muñoz and van der Laan - 2012 - Population intervention causal effects based on st.pdf:application/pdf}
}

@article{haneuseEstimationEffectInterventions2013,
	title = {Estimation of the effect of interventions that modify the received treatment},
	volume = {32},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.5907},
	doi = {10.1002/sim.5907},
	abstract = {Motivated by a study of surgical operating time and post-operative outcomes for lung cancer, we consider the estimation of causal effects of continuous point-exposure treatments. To investigate causality, the standard paradigm postulates a series of treatment-specific counterfactual outcomes and establishes conditions under which we may learn about them from observational study data. While many choices are possible, causal effects are typically defined in terms of variation of the mean of counterfactual outcomes in hypothetical worlds in which specific treatment strategies are ‘applied’ to all individuals. For example, one might compare two worlds: one where each individual receives some specific dose and a second where each individual receives some other dose. For our motivating study, defining causal effects in this way corresponds to (hypothetical) interventions that could not conceivably be implemented in the real world. In this work, we consider an alternative, complimentary framework that investigates variation in the mean of counterfactual outcomes under hypothetical treatment strategies where each individual receives a treatment dose corresponding to that actually received but modified in some pre-specified way. Quantification of this variation is defined in terms of contrasts for specific interventions as well as in terms of the parameters of a new class of marginal structural mean models. Within this framework, we propose three estimators: an outcome regression estimator, an inverse probability of treatment weighted estimator and a doubly robust estimator. We illustrate the methods with an analysis of the motivating data. Copyright © 2013 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {30},
	urldate = {2020-06-26},
	journal = {Statistics in Medicine},
	author = {Haneuse, S. and Rotnitzky, A.},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.5907},
	keywords = {causal inference, double robustness, marginal structural mean model, observational study},
	pages = {5260--5277},
	file = {Snapshot:/Users/Nick/Zotero/storage/B74RQI4U/sim.html:text/html;Full Text PDF:/Users/Nick/Zotero/storage/W3PEP8PS/Haneuse and Rotnitzky - 2013 - Estimation of the effect of interventions that mod.pdf:application/pdf}
}

@misc{TlverseSl32020,
	title = {tlverse/sl3},
	copyright = {GPL-3.0 License         ,                 GPL-3.0 License},
	url = {https://github.com/tlverse/sl3},
	abstract = {💪 🤔 Modern Super Learning using Pipelines. Contribute to tlverse/sl3 development by creating an account on GitHub.},
	urldate = {2020-06-26},
	publisher = {tlverse},
	month = jun,
	year = {2020},
	note = {original-date: 2017-08-08T21:09:22Z},
	keywords = {data-science, ensemble-learning, ensemble-model, machine-learning, model-selection, r, r-package, regression, stacking, statistics}
}

@article{breimanStackedRegressions1996,
	title = {Stacked regressions},
	volume = {24},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00117832},
	doi = {10.1007/BF00117832},
	abstract = {Stacking regressions is a method for forming linear combinations of different predictors to give improved prediction accuracy. The idea is to use cross-validation data and least squares under non-negativity constraints to determine the coefficients in the combination. Its effectiveness is demonstrated in stacking regression trees of different sizes and in a simulation stacking linear subset and ridge regressions. Reasons why this method works are explored. The idea of stacking originated with Wolpert (1992).},
	language = {en},
	number = {1},
	urldate = {2020-06-29},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = jul,
	year = {1996},
	pages = {49--64},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/HDE24XYF/Breiman - 1996 - Stacked regressions.pdf:application/pdf}
}

@article{wolpertStackedGeneralization1992,
	title = {Stacked generalization},
	volume = {5},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608005800231},
	doi = {10.1016/S0893-6080(05)80023-1},
	abstract = {This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.},
	language = {en},
	number = {2},
	urldate = {2020-06-29},
	journal = {Neural Networks},
	author = {Wolpert, David H.},
	month = jan,
	year = {1992},
	keywords = {cross-validation, Combining generalizers, Error estimation and correction, Generalization and induction, Learning set preprocessing},
	pages = {241--259},
	file = {ScienceDirect Snapshot:/Users/Nick/Zotero/storage/N8CSQUTB/S0893608005800231.html:text/html;ScienceDirect Full Text PDF:/Users/Nick/Zotero/storage/F2FD8J45/Wolpert - 1992 - Stacked generalization.pdf:application/pdf}
}

@article{laanSuperLearner2007,
	title = {Super {Learner}},
	volume = {6},
	issn = {1544-6115, 2194-6302},
	url = {https://www.degruyter.com/view/journals/sagmb/6/1/article-sagmb.2007.6.1.1309.xml.xml},
	doi = {10.2202/1544-6115.1309},
	abstract = {{\textless}section class="abstract"{\textgreater}{\textless}p{\textgreater}When trying to learn a model for the prediction of an outcome given a set of covariates, a statistician has many estimation procedures in their toolbox.  A few examples of these candidate learners are: least squares, least angle regression, random forests, and spline regression.  Previous articles (van der Laan and Dudoit (2003); van der Laan et al. (2006); Sinisi et al. (2007)) theoretically validated the use of cross validation to select an optimal learner among many candidate learners.  Motivated by this use of cross validation, we propose a new prediction method for creating a weighted combination of many candidate learners to build the super learner.  This article proposes a fast algorithm for constructing a super learner in prediction which uses V-fold cross-validation to select weights to combine an initial set of candidate learners.  In addition, this paper contains a practical demonstration of the adaptivity of this so called super learner to various true data generating distributions.   This approach for construction of a super learner generalizes to any parameter which can be defined as a minimizer of a loss function.{\textless}/p{\textgreater}{\textless}/section{\textgreater}},
	language = {en},
	number = {1},
	urldate = {2020-06-29},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Laan, Mark J. van der and Polley, Eric C. and Hubbard, Alan E.},
	month = sep,
	year = {2007},
	note = {Publisher: De Gruyter
Section: Statistical Applications in Genetics and Molecular Biology},
	file = {Full Text PDF:/Users/Nick/Zotero/storage/TPAQ95LL/Laan et al. - 2007 - Super Learner.pdf:application/pdf;Snapshot:/Users/Nick/Zotero/storage/Q3ZTPZI8/article-sagmb.2007.6.1.1309.xml.html:text/html}
}

@article{diazNonparametricCausalEffects2020a,
	title = {Non-parametric causal effects based on longitudinal modified treatment policies},
	url = {http://arxiv.org/abs/2006.01366},
	abstract = {Most causal inference methods consider counterfactual variables under interventions that set the treatment deterministically. With continuous or multi-valued treatments or exposures, such counterfactuals may be of little practical interest because no feasible intervention can be implemented that would bring them about. Furthermore, violations to the positivity assumption, necessary for identification, are exacerbated with continuous and multi-valued treatments and deterministic interventions. In this paper we propose longitudinal modified treatment policies (LMTPs) as a non-parametric alternative. LMTPs can be designed to guarantee positivity, and yield effects of immediate practical relevance with an interpretation that is familiar to regular users of linear regression adjustment. We study the identification of the LMTP parameter, study properties of the statistical estimand such as the efficient influence function, and propose four different estimators. Two of our estimators are efficient, and one is sequentially doubly robust in the sense that it is consistent if, for each time point, either an outcome regression or a treatment mechanism is consistently estimated. We perform a simulation study to illustrate the properties of the estimators, and present the results of our motivating study on hypoxemia and mortality in Intensive Care Unit (ICU) patients. Software implementing our methods is provided in the form of the open source {\textbackslash}texttt\{R\} package {\textbackslash}texttt\{lmtp\} freely available on GitHub ({\textbackslash}url\{https://github.com/nt-williams/lmtp\}).},
	urldate = {2020-07-01},
	journal = {arXiv:2006.01366},
	author = {Diaz, Ivan and Williams, Nicholas and Hoffman, Katherine L. and Schenck, Edward J.},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.01366
version: 2},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/Nick/Zotero/storage/2476RRIJ/Díaz et al. - 2020 - Non-parametric causal effects based on longitudina.pdf:application/pdf;arXiv.org Snapshot:/Users/Nick/Zotero/storage/VA3XY47I/2006.html:text/html}
}

@article{chernozhukovDoubleDebiasedMachine2018,
	title = {Double/debiased machine learning for treatment and structural parameters},
	volume = {21},
	copyright = {© 2017 Royal Economic Society.},
	issn = {1368-423X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12097},
	doi = {10.1111/ectj.12097},
	abstract = {We revisit the classic semi-parametric problem of inference on a low-dimensional parameter θ0 in the presence of high-dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high-dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross-fitting, which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an -neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
	language = {en},
	number = {1},
	urldate = {2020-07-01},
	journal = {The Econometrics Journal},
	author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12097},
	pages = {C1--C68},
	file = {Snapshot:/Users/Nick/Zotero/storage/UESTV8LX/ectj.html:text/html;Full Text PDF:/Users/Nick/Zotero/storage/SI8XEHG5/Chernozhukov et al. - 2018 - Doubledebiased machine learning for treatment and.pdf:application/pdf}
}

@article{youngIdentificationEstimationApproximation2014,
	title = {Identification, estimation and approximation of risk under interventions that depend on the natural value of treatment using observational data},
	volume = {3},
	issn = {2194-9263},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4387917/},
	doi = {10.1515/em-2012-0001},
	number = {1},
	urldate = {2020-07-01},
	journal = {Epidemiologic methods},
	author = {Young, Jessica G. and Herńan, Miguel A. and Robins, James M.},
	month = dec,
	year = {2014},
	pmid = {25866704},
	pmcid = {PMC4387917},
	pages = {1--19},
	file = {PubMed Central Full Text PDF:/Users/Nick/Zotero/storage/FXB57BRJ/Young et al. - 2014 - Identification, estimation and approximation of ri.pdf:application/pdf}
}

@article{robinsEffectsMultipleInterventions,
	title = {Effects of multiple interventions},
	language = {en},
	author = {Robins, James and Hernan, Miguel and Siebert, Uwe},
	pages = {40},
	file = {Robins et al. - Effects of multiple interventions.pdf:/Users/Nick/Zotero/storage/5SZDNLH4/Robins et al. - Effects of multiple interventions.pdf:application/pdf}
}

@article{taubmanInterveningRiskFactors2009,
	title = {Intervening on risk factors for coronary heart disease: an application of the parametric g-formula},
	volume = {38},
	issn = {0300-5771},
	shorttitle = {Intervening on risk factors for coronary heart disease},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2786249/},
	doi = {10.1093/ije/dyp192},
	abstract = {Estimating the population risk of disease under hypothetical interventions—such as the population risk of coronary heart disease (CHD) were everyone to quit smoking and start exercising or to start exercising if diagnosed with diabetes—may not be possible using standard analytic techniques. The parametric g-formula, which appropriately adjusts for time-varying confounders affected by prior exposures, is especially well suited to estimating effects when the intervention involves multiple factors (joint interventions) or when the intervention involves decisions that depend on the value of evolving time-dependent factors (dynamic interventions). We describe the parametric g-formula, and use it to estimate the effect of various hypothetical lifestyle interventions on the risk of CHD using data from the Nurses’ Health Study. Over the period 1982–2002, the 20-year risk of CHD in this cohort was 3.50\%. Under a joint intervention of no smoking, increased exercise, improved diet, moderate alcohol consumption and reduced body mass index, the estimated risk was 1.89\% (95\% confidence interval: 1.46–2.41). We discuss whether the assumptions required for the validity of the parametric g-formula hold in the Nurses’ Health Study data. This work represents the first large-scale application of the parametric g-formula in an epidemiologic cohort study.},
	number = {6},
	urldate = {2020-07-01},
	journal = {International Journal of Epidemiology},
	author = {Taubman, Sarah L and Robins, James M and Mittleman, Murray A and Hernán, Miguel A},
	month = dec,
	year = {2009},
	pmid = {19389875},
	pmcid = {PMC2786249},
	pages = {1599--1611},
	file = {PubMed Central Full Text PDF:/Users/Nick/Zotero/storage/NJ4ZVBD3/Taubman et al. - 2009 - Intervening on risk factors for coronary heart dis.pdf:application/pdf}
}

@article{rosenbaumCentralRolePropensity1983,
	title = {The {Central} {Role} of the {Propensity} {Score} in {Observational} {Studies} for {Causal} {Effects}},
	volume = {70},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2335942},
	doi = {10.2307/2335942},
	abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two-dimensional plot.},
	number = {1},
	urldate = {2020-07-01},
	journal = {Biometrika},
	author = {Rosenbaum, Paul R. and Rubin, Donald B.},
	year = {1983},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {41--55},
	file = {Full Text:/Users/Nick/Zotero/storage/MLNCWLBG/Rosenbaum and Rubin - 1983 - The Central Role of the Propensity Score in Observ.pdf:application/pdf}
}

@incollection{roseOpenProblem2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {The {Open} {Problem}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_1},
	abstract = {The debate over hormone replacement therapy (HRT) has been one of the biggest health discussions in recent history. Professional groups and nonprofits, such as the American College of Physicians and the American Heart Association, gave HRT their stamp of approval 15 years ago. Studies indicated that HRT was protective against osteoporosis and heart disease. HRT became big business, with millions upon millions of prescriptions filled each year. However, in 1998, the Heart and Estrogen-Progestin Replacement Study demonstrated increased risk of heart attack among women with heart disease taking HRT, and in 2002 the Women’s Health Initiative showed increased risk for breast cancer, heart disease, and stroke, among other ailments, for women on HRT. Why were there inconsistencies in the study results?},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Rose, Sherri and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_1},
	keywords = {Breast Cancer, Heart Disease, Hormone Replacement Therapy, Open Problem, Public Health},
	pages = {3--20},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/VEVG98BL/Rose and van der Laan - 2011 - The Open Problem.pdf:application/pdf}
}

@incollection{roseUnderstandingTMLE2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Understanding {TMLE}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_5},
	abstract = {This chapter focuses on understanding TMLE. We go into more detail than the previous chapter to demonstrate how this estimator is derived. Recall that TMLE is a two-step procedure where one first obtains an estimate of the data-generating distribution P0 or the relevant portion Q0 of P0. The second stage updates this initial fit in a step targeted toward making an optimal bias–variance tradeoff for the parameter of interest ψ(Q0), instead of the overall density P0. The procedure is double robust and can incorporate data-adaptive-likelihood-based estimation procedures to estimate Q0 and the treatment mechanism.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Rose, Sherri and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_5},
	pages = {83--100},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/8HBKZYFW/Rose and van der Laan - 2011 - Understanding TMLE.pdf:application/pdf}
}

@incollection{polleySuperLearningRightCensored2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Super {Learning} for {Right}-{Censored} {Data}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_16},
	abstract = {The super learner was introduced in Chap. 3 as a loss-based estimator of a parameter of the data-generating distribution, defined as the minimizer of the expectation (risk) of a loss function over all candidate parameter values in the parameter space. This chapter demonstrates how the super learner framework can also be applied to estimate parameters such as conditional hazards or survival functions of a failure time, given a vector of baseline covariates, based on right-censored data. We strongly advise readers to familiarize themselves with the concepts presented in Chap. 3 before reading this chapter, as we assume the reader has a firm grasp of that material.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Polley, Eric C. and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_16},
	pages = {249--258},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/RJLVB39C/Polley and van der Laan - 2011 - Super Learning for Right-Censored Data.pdf:application/pdf}
}

@incollection{zhengCrossValidatedTargetedMinimumLossBased2011b,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Cross-{Validated} {Targeted} {Minimum}-{Loss}-{Based} {Estimation}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_27},
	abstract = {In previous chapters, we introduced targeted maximum likelihood estimation in semiparametric models, which incorporates adaptive estimation (e.g., loss-based super learning) of the relevant part of the data-generating distribution and subsequently carries out a targeted bias reduction by maximizing the log-likelihood, or minimizing another loss-specific empirical risk, over a “clever” parametric working model through the initial estimator, treating the initial estimator as offset. This updating process may need to be iterated to convergence. The target parameter of the resulting updated estimator is then evaluated, and is called the targeted minimum-loss- based estimator (also TMLE) of the target parameter of the data-generating distribution. This estimator is, by definition, a substitution estimator, and, under regularity conditions, is a double robust semiparametric efficient estimator.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Zheng, Wenjing and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_27},
	pages = {459--474},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/BYS9CML6/Zheng and van der Laan - 2011 - Cross-Validated Targeted Minimum-Loss-Based Estima.pdf:application/pdf}
}

@incollection{munozTargetedBayesianLearning2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Targeted {Bayesian} {Learning}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_28},
	abstract = {TMLE is a loss-based semiparametric estimation method that yields a substitution estimator of a target parameter of the probability distribution of the data that solves the efficient influence curve estimating equation and thereby yields a double robust locally efficient estimator of the parameter of interest under regularity conditions. The Bayesian paradigm is concerned with including the researcher’s prior uncertainty about the probability distribution through a prior distribution on a statistical model for the probability distribution, which combined with the likelihood yields a posterior distribution of the probability distribution that reflects the researcher’s posterior uncertainty. Just like model-based maximum likelihood learning, Bayesian learning is intrinsically nontargeted by working with the prior and posterior distributions of the whole probability distribution of the observed data structure and is thereby very susceptible to bias due to model misspecification or nontargeted model selection.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Muñoz, Iván Díaz and Hubbard, Alan E. and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_28},
	pages = {475--493},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/AFQKQU4M/Muñoz et al. - 2011 - Targeted Bayesian Learning.pdf:application/pdf}
}

@incollection{roseDefiningModelParameter2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Defining the {Model} and {Parameter}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_2},
	abstract = {Targeted statistical learning from data is often concerned with the estimation of causal effects and an assessment of uncertainty for the estimator. In Chap. 1, we identified the road map we will follow to solve this estimation problem. Now, we formalize the concepts of the model and target parameter. We will introduce additional topics that may seem abstract. While we attempt to elucidate these abstractions with tangible examples, depending on your background, the material may be quite dense compared to other textbooks you have read. Do not get discouraged. Sometimes a second reading and careful notes are helpful and sufficient to illuminate these concepts. Researchers and students at UC Berkeley have also had great success discussing these topics in groups. If this is your assigned text for a course or workshop, meet outside of class with your fellow classmates. We guarantee you that the effort is worth it so you can move on to the next step in the targeted learning road map. Once you have a firm understanding of the core material in Chap. 2, you can begin the estimation steps.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Rose, Sherri and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_2},
	pages = {21--42},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/FHAZRZPH/Rose and van der Laan - 2011 - Defining the Model and Parameter.pdf:application/pdf}
}

@incollection{rosenblumMarginalStructuralModels2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Marginal {Structural} {Models}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_9},
	abstract = {In many applications, one would like to estimate the effect of a treatment or exposure on various subpopulations. For example, one may be interested in these questions: What is the effect of an antidepressant medication on Hamilton Depression Rating Scale (HAM-D) score for those who enter a study with severe depression, and for those who enter with moderate depression? What is the effect of a cancer therapy for those who test positive for overexpression of a particular gene and for those who test negative for overexpression of that gene? What is the impact of low adherence to antiretroviral therapy on viral load for HIV-positive individuals who have just achieved viral suppression and for those who have maintained continuous viral suppression for 1 year?},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Rosenblum, Michael},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_9},
	keywords = {Public Health, Cancer Therapy, Severe Depression, Statistical Theory, Viral Load},
	pages = {145--160},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/6X8279CH/Rosenblum - 2011 - Marginal Structural Models.pdf:application/pdf}
}

@incollection{vanderlaanFoundationsTMLE2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Foundations of {TMLE}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_30},
	abstract = {An estimator of a parameter is a mapping from the data set to the parameter space. Estimators that are empirical means of a function of the unit data structure are asymptotically consistent and normally distributed due to the CLT. Such estimators are called linear in the empirical probability distribution. Most estimators are not linear, but many are approximately linear in the sense that they are linear up to a negligible (in probability) remainder term. One states that the estimator is asymptotically linear, and the relevant function of the unit data structure, centered to have mean zero, is called the influence curve of the estimator. How does one prove that an estimator is asymptotically linear? One key step is to realize that an estimator is a mapping from a possibly very large collection of empirical means of functions of the unit data structure into the parameter space. Such a collection of empirical means is called an empirical process whose behavior with respect to uniform consistency and the uniform CLT is established in empirical process theory. In this section we present succinctly that (1) a uniform central limit theorem for the vector of empirical means, combined with (2) differentiability of the estimator as a mapping from the vector of empirical means into the parameter space yields the desired asymptotic linearity. This method for establishing the asymptotic linearity and normality of the estimator is called the functional delta method (van der Vaart and Wellner 1996; Gill 1989).},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {van der Laan, Mark J. and Rose, Sherri},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_30},
	pages = {521--583},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/37PZBR4D/van der Laan and Rose - 2011 - Foundations of TMLE.pdf:application/pdf}
}

@incollection{wangFindingQuantitativeTrait2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Finding {Quantitative} {Trait} {Loci} {Genes}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_23},
	abstract = {The goal of quantitative trait loci (QTL) mapping is to identify genes underlying an observed trait in the genome using genetic markers. In experimental organisms, the QTL mapping experiment usually involves crossing two inbred lines with substantial differences in a trait, and then scoring the trait in the segregating progeny. A series of markers along the genome is genotyped in the segregating progeny, and associations between the trait and the QTL can be evaluated using the marker information. Of primary interest are the positions and effect sizes of QTL genes.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Wang, Hui and Rose, Sherri and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_23},
	pages = {383--394},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/RZFY2XZ4/Wang et al. - 2011 - Finding Quantitative Trait Loci Genes.pdf:application/pdf}
}

@incollection{sekhonPropensityScoreBasedEstimatorsCTMLE2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Propensity-{Score}-{Based} {Estimators} and {C}-{TMLE}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_21},
	abstract = {In order to estimate the average treatment effect E0[E0(Y {\textbar} A = 1,W) - E0(Y {\textbar} A = 0,W)] of a single time-point treatment A based on observing n i.i.d. copies of O = (W, A, Y), one might use inverse probability of treatment (i.e., propensity score) weighting of an estimator of the conditional mean of the outcome (i.e., response surface) as a function of the pretreatment covariates. Alternatively, one might use a TMLE defined by a choice of initial estimator, a parametric submodel that codes fluctuations of the initial estimator, and a loss function used to determine the amount of fluctuation, where either the choice of submodel or the loss function will involve inverse probability of treatment weighting. Asymptotically, such double robust estimators may have appealing properties. They can be constructed such that if either the model of the response surface or the model of the probability of treatment assignment is correct, the estimatosr will provide a consistent estimator of the average treatment effect. And if both models are correct, the weighted estimator will be asymptotically efficient. Such estimators are called double robust and locally efficient (Robins et al. 1994, 1995; Robins and Rotnitzky 1995; van der Laan and Robins 2003).},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Sekhon, Jasjeet S. and Gruber, Susan and Porter, Kristin E. and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_21},
	pages = {343--364},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/CMQ7X4YC/Sekhon et al. - 2011 - Propensity-Score-Based Estimators and C-TMLE.pdf:application/pdf}
}

@incollection{roseWhyMatchMatched2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Why {Match}? {Matched} {Case}-{Control} {Studies}},
	isbn = {978-1-4419-9782-1},
	shorttitle = {Why {Match}?},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_14},
	abstract = {Individually matched case-control study designs are common in public health and medicine, and conditional logistic regression in a parametric statistical model is the tool most commonly used to analyze these studies. In an individually matched case-control study, the population of interest is identified, and cases are randomly sampled. Each of these cases is then matched to one or more controls based on a variable (or variables) believed to be a confounder. The main potential benefit of matching in case-control studies is a gain in efficiency, not the elimination of confounding. Therefore, when are these study designs truly beneficial?},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Rose, Sherri and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_14},
	pages = {229--238},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/D6Z6LHH2/Rose and van der Laan - 2011 - Why Match Matched Case-Control Studies.pdf:application/pdf}
}

@incollection{vanderlaanIntroductionCodeImplementation2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Introduction to {R} {Code} {Implementation}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_31},
	abstract = {This appendix includes a brief introduction to the implementation of super learning and the TMLE in R. Packages and supplementary code are posted online at http://www.targetedlearningbook.com. We conclude with a few coding guides for data structures and research questions presented in Parts II–IX. The book’sWeb site will be a continually updated resource for new code, demonstrations, and packages.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {van der Laan, Mark J. and Rose, Sherri},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_31},
	pages = {585--588},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/ZB7HJ7RH/van der Laan and Rose - 2011 - Introduction to R Code Implementation.pdf:application/pdf}
}

@incollection{rubinTargetedANCOVAEstimator2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Targeted {ANCOVA} {Estimator} in {RCTs}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_12},
	abstract = {In many randomized experiments the primary goal is to estimate the average treatment effect, defined as the difference in expected responses between subjects assigned to a treatment group and subjects assigned to a control group. Linear regression is often recommended for use in RCTs as an attempt to increase precision when estimating an average treatment effect on a (nonbinary) outcome by exploiting baseline covariates. The coefficient in front of the treatment variable is then reported as the estimate of the average treatment effect, assuming that no interactions between treatment and covariates were included in the linear regression model.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Rubin, Daniel B. and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_12},
	pages = {201--215},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/4C4YJYHD/Rubin and van der Laan - 2011 - Targeted ANCOVA Estimator in RCTs.pdf:application/pdf}
}

@incollection{petersenPositivity2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Positivity},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_10},
	abstract = {The identifiability of causal effects requires sufficient variability in treatment or exposure assignment within strata of confounders. The causal inference literature refers to the assumption of adequate exposure variability within confounder strata as the assumption of positivity or experimental treatment assignment. Positivity violations can arise for two reasons. First, it may be theoretically impossible for individuals with certain covariate values to receive a given exposure of interest. For example, certain patient characteristics may constitute an absolute contraindication to receipt of a particular treatment. The threat to causal inference posed by such structural or theoretical violations of positivity does not improve with increasing sample size. Second, violations or near violations of positivity can arise in finite samples due to chance. This is a particular problem in small samples but also occurs frequently in moderate to large samples when the treatment is continuous or can take multiple levels, or when the covariate adjustment set is large or contains continuous or multilevel covariates. Regardless of the cause, causal effects may be poorly or nonidentified when certain subgroups in a finite sample do not receive some of the treatment levels of interest. In this chapter we will use the term “sparsity” to refer to positivity violations and near-violations arising from either of these causes, recognizing that other types of sparsity can also threaten valid inference.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Petersen, Maya L. and Porter, Kristin E. and Gruber, Susan and Wang, Yue and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_10},
	pages = {161--184},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/UA59YTPX/Petersen et al. - 2011 - Positivity.pdf:application/pdf}
}

@incollection{chambazTMLEAdaptiveGroup2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {{TMLE} in {Adaptive} {Group} {Sequential} {Covariate}-{Adjusted} {RCTs}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_29},
	abstract = {This chapter is devoted to group sequential covariate-adjusted RCTs analyzed through the prism of TMLE. By adaptive covariate-adjusted design we mean an RCT group sequential design that allows the investigator to dynamically modify its course through data-driven adjustment of the randomization probability based on data accrued so far, without negatively impacting the statistical integrity of the trial. Moreover, the patient’s baseline covariates may be taken into account for the random treatment assignment. This definition is slightly adapted from Golub (2006). In particular, we assume following the definition of prespecified sampling plans given in Emerson (2006) that, prior to collection of the data, the trial protocol specifies the parameter of scientific interest, the inferential method, and confidence level to be used when constructing a confidence interval for the latter parameter.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Chambaz, Antoine and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_29},
	pages = {495--518},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/4DAGCFSJ/Chambaz and van der Laan - 2011 - TMLE in Adaptive Group Sequential Covariate-Adjust.pdf:application/pdf}
}

@incollection{stitelmanCTMLETimetoEventOutcomes2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {C-{TMLE} for {Time}-to-{Event} {Outcomes}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_20},
	abstract = {In this chapter, the C-TMLE for the treatment-specific survival curve based on rightcensored data will be presented. It is common that one wishes to assess the effect of a treatment or exposure on the time it takes for an event to occur based on an observational database. Chapters 17 and 18 discussed the treatment-specific survival curve in RCTs. The TMLE presented there improves upon common methods for analyzing time-to-event data in robustness, efficiency, and interpretability of parameter estimates. Observational data differ from RCTs in that the exposure/treatment is not set according to a known mechanism. Moreover, in situations where there is dependent censoring the censoring mechanism is also unknown. As a consequence, in observational studies, the TMLE needs to estimate the treatment and censoring mechanism, and this needs to be done in such a way that the resulting targeted bias reduction carried out by the TMLE is fully effective.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Stitelman, Ori M. and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_20},
	pages = {323--342},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/FVU3WLGA/Stitelman and van der Laan - 2011 - C-TMLE for Time-to-Event Outcomes.pdf:application/pdf}
}

@incollection{roseIndependentCaseControlStudies2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Independent {Case}-{Control} {Studies}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_13},
	abstract = {Case-control study designs are frequently used in public health and medical research to assess potential risk factors for disease. These study designs are particularly attractive to investigators researching rare diseases, as they are able to sample known cases of disease vs. following a large number of subjects and waiting for disease onset in a relatively small number of individuals.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Rose, Sherri and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_13},
	pages = {219--228},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/A2IK75JR/Rose and van der Laan - 2011 - Independent Case-Control Studies.pdf:application/pdf}
}

@incollection{petersenCaseStudyLongitudinal2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Case {Study}: {Longitudinal} {HIV} {Cohort} {Data}},
	isbn = {978-1-4419-9782-1},
	shorttitle = {Case {Study}},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_24},
	abstract = {In this chapter, we introduce a case study based on the treatment of HIV infection. A series of scientific questions concerning how best to detect and manage antiretroviral treatment failure in resource-limited settings are used to illustrate the general road map for targeted learning. We emphasize the translation of background knowledge into a formal causal and statistical model and the translation of scientific questions into target causal parameters and corresponding statistical parameters of the distribution of the observed data. Readers may be interested in first reading the longitudinal sections of Appendix A for a rigorous treatment of longitudinal TMLE and related topics.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Petersen, Maya L. and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_24},
	pages = {397--417},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/ZQYX5H9L/Petersen and van der Laan - 2011 - Case Study Longitudinal HIV Cohort Data.pdf:application/pdf}
}

@incollection{roseNestedCaseControlRisk2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Nested {Case}-{Control} {Risk} {Score} {Prediction}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_15},
	abstract = {Risk scores are calculated to identify those patients at the highest level of risk for an outcome. In some cases, interventions are implemented for patients at high risk. Standard practice for risk score prediction relies heavily on parametric regression. Generating a good estimator of the function of interest using parametric regression can be a significant challenge. As discussed in Chap. 3, high-dimensional data are increasingly common in epidemiology, and researchers may have dozens, hundreds, or thousands of potential predictors that are possibly related to the outcome.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Rose, Sherri and Fireman, Bruce and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_15},
	pages = {239--245},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/TMXCJRVB/Rose et al. - 2011 - Nested Case-Control Risk Score Prediction.pdf:application/pdf}
}

@incollection{gruberBoundedContinuousOutcomes2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Bounded {Continuous} {Outcomes}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_7},
	abstract = {This chapter presents a TMLE of the additive treatment effect on a bounded continuous outcome. A TMLE is based on a choice of loss function and a corresponding parametric submodel through an initial estimator, chosen so that the loss-functionspecific score of this parametric submodel at zero fluctuation equals or spans the efficient influence curve of the target parameter. Two such TMLEs are considered: one based on the squared error loss function with a linear regression model, and one based on a quasi-log-likelihood loss function with a logistic regression submodel. The problem with the first TMLE is highlighted: the linear regression model is not a submodel and thus does not respect global constraints implied by the statistical model. It is theoretically and practically demonstrated that the TMLE with the logistic regression submodel is more robust than a TMLE based on least squares linear regression. Some parts of this chapter assume familiarity with the core concepts, as presented in Chap. 5. The less theoretically trained reader should aim to navigate through these parts and focus on the practical implementation and importance of the presented TMLE procedure. This chapter is adapted from Gruber and van der Laan (2010b).},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Gruber, Susan and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_7},
	pages = {121--132},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/S9ZGIDWE/Gruber and van der Laan - 2011 - Bounded Continuous Outcomes.pdf:application/pdf}
}

@incollection{rosenblumRobustAnalysisRCTs2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Robust {Analysis} of {RCTs} {Using} {Generalized} {Linear} {Models}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_11},
	abstract = {It is typical in RCTs for extensive information to be collected on subjects prior to randomization. For example, age, ethnicity, socioeconomic status, history of disease, and family history of disease may be recorded. Baseline information can be leveraged to obtain more precise estimates of treatment effects than the standard unadjusted estimator. This is often done by carrying out model-based analyses at the end of the trial, where baseline variables predictive of the primary study outcome are included in the model. As shown in Moore and van der Laan (2007), such analyses have potential to improve precision and, if carried out in the appropriate manner, give asymptotically unbiased, locally efficient estimates of the marginal treatment effect even when the model used is arbitrarily misspecified.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Rosenblum, Michael},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_11},
	pages = {187--199},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/SX7BN3A3/Rosenblum - 2011 - Robust Analysis of RCTs Using Generalized Linear M.pdf:application/pdf}
}

@incollection{tuglusTargetedMethodsBiomarker2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Targeted {Methods} for {Biomarker} {Discovery}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_22},
	abstract = {The use of biomarkers in disease diagnosis and treatment has grown rapidly in recent years, as microarray and sequencing technologies capable of detecting biological signatures have become more effective research tools. In an attempt to create a level of quality assurance with respect to biological and more specifically biomarker research, the FDA has called for the development of a standard protocol for biomarker qualification (Food and Drug Administration 2006). Such a protocol would define “evidentiary” standards for biomarker usage in areas of drug development and disease treatment and provide a standardized assessment of a biomarker’s significance and biological interpretation. This is especially relevant for RCTs, where the protocol would prohibit the use of unauthenticated biomarkers to determine treatment regime, resulting in safer and more reliable treatment decisions (Food and Drug Administration 2006). Consequentially, identifying accurate and flexible analysis tools to assess biomarker importance is essential. In this chapter, we present a measure of variable importance based on a flexible semiparametric model as a standardized measure for biomarker importance. We estimate this measure with the TMLE.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Tuglus, Catherine and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_22},
	pages = {367--382},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/38L3P597/Tuglus and van der Laan - 2011 - Targeted Methods for Biomarker Discovery.pdf:application/pdf}
}

@incollection{roseIntroductionTMLE2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Introduction to {TMLE}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_4},
	abstract = {This is the second chapter in our text to deal with estimation.We started by defining the research question. This included our data, model for the probability distribution that generated the data, and the target parameter of the probability distribution of the data. We then presented the estimation of prediction functions using super learning. This leads us to the estimation of causal effects using the TMLE. This chapter introduces TMLE, and a deeper understanding of this methodology is provided in Chap. 5. Note that we use the abbreviation TMLE for targeted maximum likelihood estimation and the targeted maximum likelihood estimator. Later in this text, we discuss targeted minimum loss-based estimation, which can also be abbreviated TMLE.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Rose, Sherri and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_4},
	pages = {67--82},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/3Z7ZQA2Y/Rose and van der Laan - 2011 - Introduction to TMLE.pdf:application/pdf}
}

@incollection{polleySuperLearning2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Super {Learning}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_3},
	abstract = {This is the first chapter in our text focused on estimation within the road map for targeted learning. Now that we’ve defined the research question, including our data, the model, and the target parameter, we are ready to begin. For the estimation of a target parameter of the probability distribution of the data, such as target parameters that can be interpreted as causal effects, we implement TMLE. The first step in this estimation procedure is an initial estimate of the data-generating distribution P0, or the relevant part Q0 of P0 that is needed to evaluate the target parameter. This is the step presented in Chap. 3, and TMLE will be presented in Chaps. 4 and 5.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Polley, Eric C. and Rose, Sherri and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_3},
	pages = {43--66},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/8FYINXRP/Polley et al. - 2011 - Super Learning.pdf:application/pdf}
}

@incollection{neugebauerIndividualizedAntiretroviralInitiation2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Individualized {Antiretroviral} {Initiation} {Rules}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_26},
	abstract = {In this chapter, TMLE is illustrated with a data analysis from a longitudinal observational study to investigate “when to start” antiretroviral therapy to reduce the incidence of AIDS-defining cancer (ADC), defined as Kaposi sarcoma, non-Hodgkin’s lymphoma, or invasive cervical cancer, in a population of HIV-infected patients. A key clinical question regarding the management of HIV/AIDS is when to start combination antiretroviral therapy (ART), defined in the Department of Health and Human Services (2004) guidelines as a regimen containing three or more individual antiretroviral medications. CD4+ T-cell count levels have been the primary marker used to determine treatment eligibility, although other factors have also been considered, such as HIV RNA levels, history of an AIDS-defining illness (Centers for Disease Control and Prevention 1992), and ability of the patient to adhere to therapy. The primary outcomes considered in ART treatment guidelines described above have always been reductions in HIV-related morbidity and mortality. Until recently, however, guidelines have not considered the effect of CD4 thresholds on the risk of specific comorbidities, such as ADC. In this analysis, we therefore evaluate how different CD4-based ART initiation strategies influence the burden of ADC. We are analyzing ADC here since it is well established that these malignancies are closely linked to immunodeficiency.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Neugebauer, Romain and Silverberg, Michael J. and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_26},
	keywords = {Cervical Cancer, Initiation Strategy, Invasive Cervical Cancer, Longitudinal Observational Study, Treatment Eligibility},
	pages = {435--456},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/8YCH9H7B/Neugebauer et al. - 2011 - Individualized Antiretroviral Initiation Rules.pdf:application/pdf}
}

@incollection{gruberCTMLEAdditivePoint2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {C-{TMLE} of an {Additive} {Point} {Treatment} {Effect}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_19},
	abstract = {C-TMLE is an extension of TMLE that pursues an optimal strategy for estimation of the nuisance parameter required in the targeting step. This latter step involves maximizing an empirical criterion over a parametric working model indexed by a nuisance parameter.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Gruber, Susan and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_19},
	pages = {301--321},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/9H8W8ERV/Gruber and van der Laan - 2011 - C-TMLE of an Additive Point Treatment Effect.pdf:application/pdf}
}

@incollection{roseWhyTMLE2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Why {TMLE}?},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_6},
	abstract = {In the previous five chapters, we covered the targeted learning road map. This included presentation of the tools necessary to estimate causal effect parameters of a data-generating distribution. We illustrated these methods with a simple data structure: O = (W, A, Y) ∼ P0. Our target parameter for this example was ψ P0 = EW, 0[E0(Y {\textbar} A = 1,W) - E0(Y {\textbar} A = 0,W)],},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Rose, Sherri and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_6},
	pages = {101--118},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/KZGC329C/Rose and van der Laan - 2011 - Why TMLE.pdf:application/pdf}
}

@incollection{hubbardDirectEffectsEffect2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Direct {Effects} and {Effect} {Among} the {Treated}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_8},
	abstract = {Researchers are frequently interested in assessing the direct effect of one variable on an outcome of interest, where this effect is not mediated through a set of intermediate variables. In this chapter, we will examine direct effects in a gender salary equity study example. Such studies provide one measure of the equity of procedures used to set salaries and of decisions in promoting and advancing faculty based on performance measures. The goal is to assess whether gender, as determined at birth, has a direct effect on the salary at which a faculty member is hired, not mediated through intermediate performance of the subject up until the time the subject gets hired. If such a direct effect exists, then that means the salary was set in response not only to merit but also to the gender of the person, indicating a gender inequality issue.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Hubbard, Alan E. and Jewell, Nicholas P. and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_8},
	pages = {133--143},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/E836PT33/Hubbard et al. - 2011 - Direct Effects and Effect Among the Treated.pdf:application/pdf}
}

@incollection{chambazProbabilitySuccessVitro2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Probability of {Success} of an {In} {Vitro} {Fertilization} {Program}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_25},
	abstract = {About 9 to 15\% of couples have difficulty in conceiving a child, i.e., do not conceive within 12 months of attempting pregnancy (Boivin et al. 2007). In response to subfertility, assisted reproductive technology has developed over the last 30 years, resulting in in vitro fertilization (IVF) techniques (the first “test-tube baby” was born in 1978). Nowadays, more than 40,000 IVF cycles are performed each year in France and more than 63,000 in the USA (Adamson et al. 2006). Yet, how to quantify the success in assisted reproduction still remains a matter of debate. One could, for instance, rely on the number of pregnancies or deliveries per IVF cycle. However, an IVF program often consists of several successive IVF cycles. So, instead of considering each IVF cycle separately, one could rather rely on an evaluation of the whole program. IVF programs are emotionally and physically burdensome. Providing the patients with the most adequate and accurate measure of success is therefore an important issue that we propose to address in this chapter.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Chambaz, Antoine},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_25},
	pages = {419--434},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/KDD4IT4Z/Chambaz - 2011 - Probability of Success of an In Vitro Fertilizatio.pdf:application/pdf}
}

@incollection{stitelmanRCTsTimetoEventOutcomes2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {{RCTs} with {Time}-to-{Event} {Outcomes} and {Effect} {Modification} {Parameters}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_18},
	abstract = {Current methods used to evaluate effect modification in time-to-event data, such as the Cox proportional hazards model or its discrete time analog the logistic failure time model, posit highly restrictive parametric statistical models and attempt to estimate parameters that are specific to the model proposed. These methods, as a result of their parametric nature, tend to be biased and force practitioners to estimate parameters that are convenient rather than parameters they are actually interested in estimating. The TMLE improves on the currently implemented methods in both robustness, its ability to provide unbiased estimates, and flexibility, allowing practitioners to estimate parameters that directly answer their question of interest.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Stitelman, Ori M. and De Gruttola, Victor and Wester, C. William and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_18},
	keywords = {Public Health, Statistical Theory, Discrete Time, Hazard Model, Statistical Model},
	pages = {271--298},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/IWQUA6CV/Stitelman et al. - 2011 - RCTs with Time-to-Event Outcomes and Effect Modifi.pdf:application/pdf}
}

@incollection{mooreRCTsTimetoEventOutcomes2011,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {{RCTs} with {Time}-to-{Event} {Outcomes}},
	isbn = {978-1-4419-9782-1},
	url = {https://doi.org/10.1007/978-1-4419-9782-1_17},
	abstract = {RCTs are often designed with the goal of investigating a causal effect of a new treatment drug vs. the standard of care on a time-to-event outcome. Possible outcomes are time to death, time to virologic failure, and time to recurrence of cancer. The data collected on a subject accumulates over time until the minimum of the time of analysis (end of study), the time the subject drops out of the study, or until the event of interest is observed. Typically, for a large proportion of the subjects recruited into the trial, the subject is right censored before the event of interest is observed, i.e., the time of analysis or the time the subject drops out of the study occurs before the time until the event of interest. The dropout time of the subject can be related to the actual time to failure one would have observed if the person had not dropped out prematurely. In this case, the standard unadjusted estimator of a causal effect of treatment on a survival time, such as the difference of the treatment-specific Kaplan–Meier survival curves at a particular point in time, is not only inefficient by not utilizing the available covariate information, but it is also biased due to informative dropout.},
	language = {en},
	urldate = {2020-08-24},
	booktitle = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	publisher = {Springer},
	author = {Moore, Kelly L. and van der Laan, Mark J.},
	editor = {van der Laan, Mark J. and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1_17},
	pages = {259--269},
	file = {Springer Full Text PDF:/Users/Nick/Zotero/storage/63D5XEY7/Moore and van der Laan - 2011 - RCTs with Time-to-Event Outcomes.pdf:application/pdf}
}

@misc{TargetedLearningCausal,
	title = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data} {\textbar} {Request} {PDF}},
	shorttitle = {Targeted {Learning}},
	url = {https://www.researchgate.net/publication/321610494_Targeted_Learning_Causal_Inference_for_Observational_and_Experimental_Data},
	abstract = {Request PDF {\textbar} Targeted Learning: Causal Inference for Observational and Experimental Data {\textbar} The statistics profession is at a unique point in history. The need for valid statistical tools is greater than ever; data sets are massive, often... {\textbar} Find, read and cite all the research you need on ResearchGate},
	language = {en},
	urldate = {2020-08-24},
	journal = {ResearchGate},
	note = {ISBN: 9781441997821},
	file = {Snapshot:/Users/Nick/Zotero/storage/U3UFH9T7/321610494_Targeted_Learning_Causal_Inference_for_Observational_and_Experimental_Data.html:text/html}
}

@book{laanTargetedLearningCausal2011a,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Targeted {Learning}: {Causal} {Inference} for {Observational} and {Experimental} {Data}},
	isbn = {978-1-4419-9781-4},
	shorttitle = {Targeted {Learning}},
	url = {https://www.springer.com/us/book/9781441997814},
	abstract = {The statistics profession is at a unique point in history. The need for valid statistical tools is greater than ever; data sets are massive, often measuring hundreds of thousands of measurements for a single subject. The field is ready to move towards clear objective benchmarks under which tools can be evaluated. Targeted learning allows (1) the full generalization and utilization of cross-validation as an estimator selection tool so that the subjective choices made by humans are now made by the machine, and (2) targeting the fitting of the probability distribution of the data toward the target parameter representing the scientific question of interest. This book is aimed at both statisticians and applied researchers interested in causal inference and general effect estimation for observational and experimental data. Part I is an accessible introduction to super learning and the targeted maximum likelihood estimator, including related concepts necessary to understand and apply these methods. Parts II-IX handle complex data structures and topics applied researchers will immediately recognize from their own research, including time-to-event outcomes, direct and indirect effects, positivity violations, case-control studies, censored data, longitudinal data, and genomic studies."Targeted Learning, by Mark J. van der Laan and Sherri Rose, fills a much needed gap in statistical and causal inference. It protects us from wasting computational, analytical, and data resources on irrelevant aspects of a problem and teaches us how to focus on what is relevant – answering questions that researchers truly care about."-Judea Pearl, Computer Science Department, University of California, Los Angeles"In summary, this book should be on the shelf of every investigator who conducts observational research and randomized controlled trials. The concepts and methodology are foundational for causal inference and at the same time stay true to what the data at hand can say about the questions that motivate their collection."-Ira B. Tager, Division of Epidemiology, University of California, Berkeley},
	language = {en},
	urldate = {2020-08-24},
	publisher = {Springer-Verlag},
	author = {Laan, Mark J. van der and Rose, Sherri},
	year = {2011},
	doi = {10.1007/978-1-4419-9782-1},
	file = {Snapshot:/Users/Nick/Zotero/storage/MZGHNI6T/9781441997814.html:text/html}
}

@article{laanTargetedMaximumLikelihood2006,
	title = {Targeted {Maximum} {Likelihood} {Learning}},
	volume = {2},
	issn = {1557-4679},
	url = {https://www.degruyter.com/view/journals/ijb/2/1/article-ijb.2006.2.1.1043.xml.xml},
	doi = {10.2202/1557-4679.1043},
	abstract = {{\textless}section class="abstract"{\textgreater}{\textless}p{\textgreater}Suppose one observes a sample of independent and identically distributed observations from a particular data generating distribution.  Suppose that one is concerned with estimation of a particular pathwise differentiable Euclidean parameter. A substitution estimator evaluating the parameter of a given likelihood based density estimator is typically too biased and might not even converge at the parametric rate: that is, the density estimator was targeted to be a good estimator of the density and might therefore result in  a poor estimator of a particular smooth functional of the density. In this article we propose a one step (and, by iteration, k-th step) targeted maximum likelihood density estimator which involves 1) creating a hardest parametric submodel with parameter epsilon through the given density estimator with score  equal to the efficient influence curve of the pathwise differentiable parameter at the density estimator,  2) estimating epsilon with the maximum likelihood estimator, and 3) defining a new density estimator as the corresponding update of the original density estimator. We show that iteration of this algorithm results in a targeted maximum likelihood density estimator which solves the efficient influence curve estimating equation and thereby yields a locally efficient estimator of the parameter of interest, under regularity conditions. In particular, we show that, if the parameter is linear and the model is convex, then the targeted maximum likelihood estimator is often achieved in the first step, and it results in a locally efficient estimator at an arbitrary (e.g., heavily misspecified) starting density.We also show that the targeted maximum likelihood estimators are now in full agreement with the locally efficient estimating function methodology as presented in Robins and Rotnitzky (1992) and van der Laan and Robins (2003), creating, in particular, algebraic equivalence between the double robust locally efficient estimators using the targeted maximum likelihood estimators as an estimate of its nuisance parameters, and targeted maximum likelihood estimators. In addition, it is argued that the targeted MLE has various advantages relative to the current estimating function based approach. We proceed by providing data driven methodologies to select the initial density estimator for the targeted MLE, thereby providing data adaptive targeted maximum likelihood estimation methodology.  We illustrate the method with various worked out examples.{\textless}/p{\textgreater}{\textless}/section{\textgreater}},
	language = {en},
	number = {1},
	urldate = {2020-08-24},
	journal = {The International Journal of Biostatistics},
	author = {Laan, Mark J. van der and Rubin, Daniel},
	month = dec,
	year = {2006},
	note = {Publisher: De Gruyter
Section: The International Journal of Biostatistics},
	file = {Full Text PDF:/Users/Nick/Zotero/storage/7M4ZTTJ8/Laan and Rubin - 2006 - Targeted Maximum Likelihood Learning.pdf:application/pdf;Snapshot:/Users/Nick/Zotero/storage/YQ9IS6EG/article-ijb.2006.2.1.1043.xml.html:text/html}
}

@article{qinInferencesCaseControlSemiparametric1998,
	title = {Inferences for {Case}-{Control} and {Semiparametric} {Two}-{Sample} {Density} {Ratio} {Models}},
	volume = {85},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2337391},
	abstract = {We consider inference in general binary response regression models under retrospective sampling plans. Prentice \& Pyke (1979) discovered that inference for the odds-ratio parameter in a logistic model can be based on a prospective likelihood even though the sampling scheme is retrospective. We show that the estimating function obtained from the prospective likelihood is optimal in a class of unbiased estimating functions. Also we link case-control sampling with a two-sample biased sampling problem, where the ratio of two densities is assumed to take a known parametric form. Connections between this model and the Cox proportional hazards model are pointed out. Large and small sample size behaviour of the proposed estimators is studied.},
	number = {3},
	urldate = {2020-08-25},
	journal = {Biometrika},
	author = {Qin, Jing},
	year = {1998},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {619--630}
}

@article{chengSemiparametricDensityEstimation2004,
	title = {Semiparametric {Density} {Estimation} under a {Two}-{Sample} {Density} {Ratio} {Model}},
	volume = {10},
	issn = {1350-7265},
	url = {https://www.jstor.org/stable/3318817},
	abstract = {A semiparametric density estimation is proposed under a two-sample density ratio model. This model, arising naturally from case-control studies and logistic discriminant analyses, can also be regarded as a biased sampling model. Our proposed density estimate is therefore an extension of the kernel density estimate suggested by Jones for length-biased data. We show that under the model considered the new density estimator not only is consistent but also has the 'smallest' asymptotic variance among general nonparametric density estimators. We also show how to use the new estimate to define a procedure for testing the goodness of fit of the density ratio model. Such a test is consistent under very general alternatives. Finally, we present some results from simulations and from the analysis of two real data sets.},
	number = {4},
	urldate = {2020-08-25},
	journal = {Bernoulli},
	author = {Cheng, K. F. and Chu, C. K.},
	year = {2004},
	note = {Publisher: International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability},
	pages = {583--604}
}

@article{laanSuperLearner2007a,
	title = {Super {Learner}},
	volume = {6},
	issn = {1544-6115, 2194-6302},
	url = {https://www.degruyter.com/view/journals/sagmb/6/1/article-sagmb.2007.6.1.1309.xml.xml},
	doi = {10.2202/1544-6115.1309},
	abstract = {{\textless}section class="abstract"{\textgreater}{\textless}p{\textgreater}When trying to learn a model for the prediction of an outcome given a set of covariates, a statistician has many estimation procedures in their toolbox.  A few examples of these candidate learners are: least squares, least angle regression, random forests, and spline regression.  Previous articles (van der Laan and Dudoit (2003); van der Laan et al. (2006); Sinisi et al. (2007)) theoretically validated the use of cross validation to select an optimal learner among many candidate learners.  Motivated by this use of cross validation, we propose a new prediction method for creating a weighted combination of many candidate learners to build the super learner.  This article proposes a fast algorithm for constructing a super learner in prediction which uses V-fold cross-validation to select weights to combine an initial set of candidate learners.  In addition, this paper contains a practical demonstration of the adaptivity of this so called super learner to various true data generating distributions.   This approach for construction of a super learner generalizes to any parameter which can be defined as a minimizer of a loss function.{\textless}/p{\textgreater}{\textless}/section{\textgreater}},
	language = {en},
	number = {1},
	urldate = {2020-08-25},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Laan, Mark J. van der and Polley, Eric C. and Hubbard, Alan E.},
	month = sep,
	year = {2007},
	note = {Publisher: De Gruyter
Section: Statistical Applications in Genetics and Molecular Biology},
	file = {Full Text PDF:/Users/Nick/Zotero/storage/6ZXSI4YK/Laan et al. - 2007 - Super Learner.pdf:application/pdf;Snapshot:/Users/Nick/Zotero/storage/TPJZVLU9/article-sagmb.2007.6.1.1309.xml.html:text/html}
}

@article{buckleyLinearRegressionCensored1979,
	title = {Linear {Regression} with {Censored} {Data}},
	volume = {66},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2335161},
	doi = {10.2307/2335161},
	abstract = {We give a method of estimating parameters in the linear regression model which allows the dependent variable to be censored and the residual distribution to be unspecified. The method differs from that of Miller (1976) in that the normal equations rather than the sum of squares of residuals are modified and this appears to overcome the inconsistency problems in Miller's approach. Large sample properties of the estimator of slope are derived heuristically and substantiated by simulations. Some of the heart transplant data reported and analysed by Miller are reanalysed using the present method.},
	number = {3},
	urldate = {2020-08-27},
	journal = {Biometrika},
	author = {Buckley, Jonathan and James, Ian},
	year = {1979},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {429--436}
}

@article{fanCensoredRegressionLocal1994,
	title = {Censored {Regression}: {Local} {Linear} {Approximations} and {Their} {Applications}},
	volume = {89},
	issn = {0162-1459},
	shorttitle = {Censored {Regression}},
	url = {https://www.jstor.org/stable/2290859},
	doi = {10.2307/2290859},
	abstract = {Various statistical tools are available for modeling the relationship between response and covariate if the data are fully observable. In the situation of censored data, however, those tools are no longer directly applicable. This article provides an easily implemented methodology for modeling the association, based on censored data. The form of the regression relationship will be completely determined by the data; no assumptions are made about this form. Basic ideas behind the methodology are to transform the observed data in an appropriate simple way and then to apply a locally weighted least squares regression. The proposed estimator involves a variable bandwidth that automatically adapts to the design of the data points. That the methodology is very easy to implement is illustrated by several examples, including simulation studies and an analysis of the Stanford Heart Transplant Data and the Primary Biliary Cirrhosis Data. Several theoretical considerations are reflected in the examples. Finally, some basic asymptotic results are established.},
	number = {426},
	urldate = {2020-08-27},
	journal = {Journal of the American Statistical Association},
	author = {Fan, Jianqing and Gijbels, Irene},
	year = {1994},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {560--570}
}

@misc{laanUnifiedCrossValidationMethodology2003,
	title = {Unified {Cross}-{Validation} {Methodology} {For} {Selection} {Among} {Estimators} and a {General} {Cross}-{Validated} {Adaptive} {Epsilon}-{Net} {Estimator}: {Finite} {Sample} {Oracle} {Inequalities} and {Examples}},
	shorttitle = {Unified {Cross}-{Validation} {Methodology} {For} {Selection} {Among} {Estimators} and a {General} {Cross}-{Validated} {Adaptive} {Epsilon}-{Net} {Estimator}},
	url = {/paper/Unified-Cross-Validation-Methodology-For-Selection-Laan-Dudoit/8e3cfaa13b97d152833b429b179eed389394eecf},
	abstract = {In Part I of this article we propose a general cross-validation criterian for selecting among a collection of estimators of a particular parameter of interest based on n i.i.d. observations. It is assumed that the parameter of interest minimizes the expectation (w.r.t. to the distribution of the observed data structure) of a particular loss function of a candidate parameter value and the observed data structure, possibly indexed by a nuisance parameter. The proposed cross-validation criterian is defined as the empirical mean over the validation sample of the loss function at the parameter estimate based on the training sample, averaged over random splits of the observed sample. The cross-validation selector is now the estimator which minimizes this cross-validation criterion. We illustrate that this general methodology covers, in particular, the selection problems in the current literature, but results in a wide range of new selection methods. We prove a finite sample oracle inequality, and asymptotic optimality of the cross-validated selector under general conditions. The asymptotic optimality states that the cross-validation selector performs asymptotically exactly as well as the selector which for each given data set makes the best choice (knowing the true data generating distribution). Our general framework allows, in particular, the situation in which the observed data structure is a censored version of the full data structure of interest, and where the parameter of interest is a parameter of the full data structure distribution. As examples of the parameter of the full data distribution we consider a density of (a part of) the full data structure, a conditional expectation of an outcome, given explanatory variables, a marginal survival function of a failure time, and multivariate conditional expectation of an outcome vector, given covariates. In part II of this article we show that the general estimating function methodology for censored data structures as provided in van der Laan, Robins (2002) yields the wished loss functions for the selection among estimators of a full-data distribution parameter of interest based on censored data. The corresponding cross-validation selector generalizes any of the existing selection methods in regression and density estimation (including model selection) to the censored data case. Under general conditions, our optimality results now show that the corresponing cross-validation selector performs asymptotically exactly as well as the selector which for each given data set makes the best choice (knowing the true full data distribution). In Part III of this article we propose a general estimator which is defined as follows. For a collection of subspaces and the complete parameter space, one defines an epsilon-net (i.e., a finite set of points whose epsilon-spheres cover the complete parameter space). For each epsilon and subspace one defines now a corresponding minimum cross-valided empirical risk estimator as the minimizer of cross-validated risk over the subspace-specific epsilon-net. In the special case that the loss function has no nuisance parameter, which thus covers the classical regression and density estimation cases, this epsilon and subspace specific minimum risk estimator reduces to the minimizer of the empirical risk over the corresponding epsilon-net. Finally, one selects epsilon and the subspace with the cross-validation selector. We refer to the resulting estimator as the cross-validated adaptive epsilon-net estimator. We prove an oracle inequality for this estimator which implies that the estimator minimax adaptive in the sense that it achieves the minimax optimal rate of convergence for the smallest of the guessed subspaces containing the true parameter value. Cross-Validation for Estimator Selection 1 Stating the Selection Problem. Let O1, . . . , On be n i.i.d. observations of O ∼ P0, where P0 is known to be an element of a statistical model M. Let ψ0(·) = ψ(· {\textbar} P0) be a parameter (function) of P0 of interest. Let the parameter set for this parameter be Ψ = \{ψ(· {\textbar} P ) : P ∈ M\}. Let (O,ψ) → L(O,ψ {\textbar} η0) ∈ IR be a “loss function”, possibly depending on a nuisance parameter η0 = η(P0), which maps a candidate parameter value ψ and observation O into a real number, whose expectation is minimized at ψ0: ψ0 = argminψ∈Ψ ∫ L(o, ψ {\textbar} η0)dP0(o) (1) = argminψ∈ΨE0L(O,ψ {\textbar} η0). Let Pn be the empirical distribution of O1, . . . , On. Let ψk(·) = ψk(· {\textbar} Pn) ∈ Ψ, k = 1, . . . , K(n), be a collection of estimators (i.e., algorithms one can apply to data) of ψ0(·). The choice of loss function. Different choices of loss functions can satisfy (1). In fact, (1) can define a class of possible loss functions. Different choices of loss functions result in estimators of ψ0 with different behavior. Consequently, the choice of loss function is an interesting issue to be addressed. We suggest the following reasonable strategy for selecting a loss function. Firstly, among the loss functions identifying ψ0 as the minimizer of its risk (i.e., satisfying (1), one wishes to choose a loss function which identifies the wished measure of performance/Risk θ(ψ {\textbar} P0) ≡ ∫ L(O,ψ {\textbar} η0)dP0(O) for a candidate ψ ∈ Ψ. Identifying such a function θ(ψ {\textbar} P0) on the parameter set Ψ does still not uniquely identify the loss function L(O,ψ {\textbar} η0). Secondly, given this function θ(ψ {\textbar} P0), we now wish to choose the loss function so that for a locally consistent estimator ηn of η0, 1/n ∑ i L(Oi, ψ {\textbar} ηn) is a locally efficient estimator of θ(ψ {\textbar} P0). That is, let L(O,ψ {\textbar} η0) be a parametrization of},
	language = {en},
	urldate = {2020-08-27},
	author = {Laan, M. and Dudoit, S.},
	year = {2003},
	file = {Snapshot:/Users/Nick/Zotero/storage/2WXSNSE2/8e3cfaa13b97d152833b429b179eed389394eecf.html:text/html}
}

@article{vanderlaanUnifiedCrossValidationMethodology2003,
	title = {Unified {Cross}-{Validation} {Methodology} {For} {Selection} {Among} {Estimators} and a {General} {Cross}-{Validated} {Adaptive} {Epsilon}-{Net} {Estimator}: {Finite} {Sample} {Oracle} {Inequalities} and {Examples}},
	shorttitle = {Unified {Cross}-{Validation} {Methodology} {For} {Selection} {Among} {Estimators} and a {General} {Cross}-{Validated} {Adaptive} {Epsilon}-{Net} {Estimator}},
	url = {https://biostats.bepress.com/ucbbiostat/paper130},
	journal = {U.C. Berkeley Division of Biostatistics Working Paper Series},
	author = {van der Laan, Mark and Dudoit, Sandrine},
	month = nov,
	year = {2003},
	file = {"Unified Cross-Validation Methodology For Selection Among Estimators an" by Mark J. van der Laan and Sandrine Dudoit:/Users/Nick/Zotero/storage/89LHAEW9/paper130.html:text/html}
}

@article{rotnitzkyDoublyRobustEstimation2006,
	title = {Doubly {Robust} {Estimation} of the {Area} {Under} the {Receiver}-{Operating} {Characteristic} {Curve} in the {Presence} of {Verification} {Bias}},
	volume = {101},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214505000001339},
	doi = {10.1198/016214505000001339},
	language = {en},
	number = {475},
	urldate = {2020-08-27},
	journal = {Journal of the American Statistical Association},
	author = {Rotnitzky, Andrea and Faraggi, David and Schisterman, Enrique},
	month = sep,
	year = {2006},
	pages = {1276--1288},
	file = {Rotnitzky et al. - 2006 - Doubly Robust Estimation of the Area Under the Rec.pdf:/Users/Nick/Zotero/storage/9Q7J4EAV/Rotnitzky et al. - 2006 - Doubly Robust Estimation of the Area Under the Rec.pdf:application/pdf}
}

@article{rubinDoublyRobustCensoring2006,
	title = {Doubly {Robust} {Censoring} {Unbiased} {Transformations}},
	url = {https://biostats.bepress.com/ucbbiostat/paper208},
	journal = {U.C. Berkeley Division of Biostatistics Working Paper Series},
	author = {Rubin, Daniel and Laan, Mark van der},
	month = jun,
	year = {2006},
	file = {"Doubly Robust Censoring Unbiased Transformations" by Daniel Rubin and Mark J. van der Laan:/Users/Nick/Zotero/storage/QGGYFIZY/paper208.html:text/html}
}

@article{kennedyNonparametricMethodsDoubly2017,
	title = {Nonparametric methods for doubly robust estimation of continuous treatment effects},
	volume = {79},
	issn = {1369-7412},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5627792/},
	doi = {10.1111/rssb.12212},
	abstract = {Continuous treatments (e.g., doses) arise often in practice, but many available causal effect estimators are limited by either requiring parametric models for the effect curve, or by not allowing doubly robust covariate adjustment. We develop a novel kernel smoothing approach that requires only mild smoothness assumptions on the effect curve, and still allows for misspecification of either the treatment density or outcome regression. We derive asymptotic properties and give a procedure for data-driven bandwidth selection. The methods are illustrated via simulation and in a study of the effect of nurse staffing on hospital readmissions penalties.},
	number = {4},
	urldate = {2020-08-27},
	journal = {Journal of the Royal Statistical Society. Series B, Statistical methodology},
	author = {Kennedy, Edward H. and Ma, Zongming and McHugh, Matthew D. and Small, Dylan S.},
	month = sep,
	year = {2017},
	pmid = {28989320},
	pmcid = {PMC5627792},
	pages = {1229--1245},
	file = {PubMed Central Full Text PDF:/Users/Nick/Zotero/storage/339HNEZ5/Kennedy et al. - 2017 - Nonparametric methods for doubly robust estimation.pdf:application/pdf}
}

  @Manual{coyleSl3,
    title = {\pkg{sl3}: Pipelines for Machine Learning and {Super Learning}},
    author = {Jeremy R Coyle and Nima S Hejazi and Ivana Malenica and Oleg Sofrygin},
    year = {2020},
    note = {\proglang{R} package version 1.3.8},
    doi = {10.5281/zenodo.1342293},
    url = {https://github.com/tlverse/sl3},
  }

  @Article{wrightRanger,
    title = {\pkg{ranger}: A Fast Implementation of Random Forests for High Dimensional Data in \proglang{C++} and \proglang{R}},
    author = {Marvin N. Wright and Andreas Ziegler},
    journal = {Journal of Statistical Software},
    year = {2017},
    volume = {77},
    number = {1},
    pages = {1--17},
    doi = {10.18637/jss.v077.i01},
  }
  
  @Manual{milborrowEarth,
    title = {\pkg{earth}: Multivariate Adaptive Regression Splines},
    author = {Stephen Milborrow},
    year = {2019},
    note = {\proglang{R} package version 5.1.2},
    url = {https://CRAN.R-project.org/package=earth},
  }
  
  @Manual{future,
    title = {\pkg{future}: Unified Parallel and Distributed Processing in \proglang{R} for Everyone},
    author = {Henrik Bengtsson},
    year = {2020},
    note = {\proglang{R} package version 1.18.0},
    url = {https://CRAN.R-project.org/package=future},
  }
  
  @Manual{progressr,
    title = {\pkg{progressr}: A Inclusive, Unifying API for Progress Updates},
    author = {Henrik Bengtsson},
    year = {2020},
    note = {\proglang{R} package version 0.6.0},
    url = {https://CRAN.R-project.org/package=progressr},
  }
  
  @Manual{broom,
    title = {\pkg{broom}: Convert Statistical Analysis Objects into Tidy Tibbles},
    author = {David Robinson and Alex Hayes},
    year = {2020},
    note = {\proglang{R} package version 0.5.6},
    url = {https://CRAN.R-project.org/package=broom},
  }
